Documentation Source:
cloud.google.com/bigquery/docs/data-canvas.html

Documentation Title:
Analyze with data canvas  |  BigQuery  |  Google Cloud

Documentation Content:
For this example, we are going to
select the <code>bigquery-public-data.usa_names.usa_1910_current</code>table.</p><p>Click <strong>Query</strong>to query the data. Enter a prompt to query the data.</p><strong>Potential result</strong><p>BigQuery data canvas generates the following query:</p><pre>SELECT
  state,
  gender,
  year,
  name,
  number
FROM
  `bigquery-public-data.usa_names.usa_1910_current`
</pre><p>BigQuery data canvas generates the results of the query. We are going to ask an
additional query to filter this data. Click <strong>Query these results</strong>.</p><strong>Prompt 2</strong><pre>Get me the top 10 most popular names in 1980</pre><strong>Potential result</strong><p>BigQuery data canvas generates the following query:</p><pre>SELECT
  name,
  SUM(number) AS total_count
FROM
  `bigquery-public-data`.usa_names.usa_1910_current
WHERE
  year = 1980
GROUP BY
  name
ORDER BY
  total_count DESC
LIMIT
  10;
</pre><p>After running the query, we get a table with the ten most common names of
children born in 1980.</p><p>For this example, we are going to visualize these results. Click <strong>Visualize</strong>.
BigQuery data canvas suggests several visualization options, including bar chart, pie
chart, line graph, and custom visualization. Click <strong>Create bar chart</strong>.</p><p>BigQuery data canvas creates a bar chart similar to the following:</p><p>Along with providing a chart, BigQuery data canvas summarizes some of the key details
of the data backing the visualization.



Documentation Source:
cloud.google.com/bigquery/docs/working-with-time-series.html

Documentation Title:
Work with time series data  |  BigQuery  |  Google Cloud

Documentation Content:
|    94105 | 152 |          62 |
 | 2020-09-08 18:00:00 |    94105 | 152 |          67 |
 | 2020-09-08 21:00:00 |    94105 | 143 |          74 |
 +---------------------+----------+-----+-------------*/
</code><h3>Get a 3-hour minimum and maximum value</h3><p>In the following query, you compute 3-hour minimum and maximum temperatures for
each zip code:</p><code>SELECT
  TIMESTAMP_BUCKET(time, INTERVAL 3 HOUR) AS time,
  zip_code,
  MIN(temperature) AS temperature_min,
  MAX(temperature) AS temperature_max,
FROM mydataset.environmental_data_hourly
GROUP BY zip_code, time
ORDER BY zip_code, time;

/*---------------------+----------+-----------------+-----------------+
 |        time         | zip_code | temperature_min | temperature_max |
 +---------------------+----------+-----------------+-----------------+
 | 2020-09-08 00:00:00 |    60606 |              60 |              66 |
 | 2020-09-08 03:00:00 |    60606 |              57 |              59 |
 | 2020-09-08 06:00:00 |    60606 |              55 |              56 |
 | 2020-09-08 09:00:00 |    60606 |              55 |



Documentation Source:
cloud.google.com/bigquery/docs/logistic-regression-prediction.html

Documentation Title:
Build and use a classification model on census data  |  BigQuery  |  Google Cloud

Documentation Content:
The retailer can then use a
binary logistic regression model that uses this customer information to predict
which label best represents each customer.</p><p>In this tutorial, you create a binary logistic regression model that predicts
whether a US Census respondent's income falls into one of two ranges based on
the respondent's demographic attributes.</p><h2>Create a dataset</h2><p>Create a BigQuery dataset to store your model:</p><ol><li><p>In the Google Cloud console, go to the <strong>BigQuery</strong>page.</p><a>Go to BigQuery</a></li><p>In the <strong>Explorer</strong>pane, click your project name.</p><p>Click <span>more_vert</span><strong>View actions &gt; Create dataset</strong>.</p><li><p>On the <strong>Create dataset</strong>page, do the following:</p><ul><p>For <strong>Dataset ID</strong>, enter <code>census</code>.</p><li><p>For <strong>Location type</strong>, select <strong>Multi-region</strong>, and then select <strong>US (multiple regions in United States)</strong>.</p><p>The public datasets are stored in the
<code>US</code><a>multi-region</a>. For
simplicity, store your dataset in the same location.</p></li><p>Leave the remaining default settings as they are, and click <strong>Create dataset</strong>.</p></ul></li></ol><h2>Examine the data</h2><p>Examine the dataset and identify which columns to use as
training data for the logistic regression model.



Documentation Source:
cloud.google.com/bigquery/docs/geospatial-data.html

Documentation Title:
Working with geospatial data  |  BigQuery  |  Google Cloud

Documentation Content:
For example:</p><pre>-- how many stations within 1 mile range of each zip code?
SELECT
    zip_code AS zip,
    ANY_VALUE(zip_code_geom) AS polygon,
    COUNT(*) AS bike_stations
FROM
    `bigquery-public-data.new_york.citibike_stations` AS bike_stations,
    `bigquery-public-data.geo_us_boundaries.zip_codes` AS zip_codes
WHERE ST_DWithin(
         zip_codes.zip_code_geom,
         ST_GeogPoint(bike_stations.longitude, bike_stations.latitude),
         1609.34)
GROUP BY zip
ORDER BY bike_stations DESC
</pre><p>Spatial joins perform better when your geography data is persisted. The example
above creates the geography values in the query. It is more performant to store
the geography values in a BigQuery table.</p><p>For example, the following query retrieves longitude, latitude pairs and
converts them to geographic points. When you run this query, you specify a new
destination table to store the query results:</p><pre>SELECT
  *,
  ST_GeogPoint(pLongitude, pLatitude) AS p
FROM
  mytable
</pre><p>BigQuery implements optimized spatial JOINs for INNER JOIN and
CROSS JOIN operators with the following GoogleSQL predicate functions:</p><ul><code>ST_DWithin</code><code>ST_Intersects</code><code>ST_Contains</code><code>ST_Within</code><code>ST_Covers</code><code>ST_CoveredBy</code><code>ST_Equals</code><code>ST_Touches</code></ul><p>Spatial joins are not optimized:</p><ul><li>For LEFT, RIGHT or FULL OUTER joins</li><li>In cases involving ANTI joins</li><li>When the spatial predicate is negated</li></ul><p>A JOIN that uses the <code>ST_DWithin</code>predicate is optimized only when
the distance parameter is a constant expression.</p><h2>Exporting spatial data</h2><p>When you export spatial data from BigQuery, <code>GEOGRAPHY</code>column
values are always formatted as WKT strings.



