Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-use.txt

Documentation Title:
Using the Spark Connector | Snowflake Documentation

Documentation Content:
import
java.sql.Connection
;
import
java.sql.DriverManager
;
import
java.sql.ResultSet
;
import
java.sql.ResultSetMetaData
;
import
java.sql.SQLException
;
import
java.sql.Statement
;
import
java.util.Properties
;
public
class
SnowflakeJDBCExample
{
public
static
void
main
(
String
[]
args
)
throws
Exception
{
String
jdbcUrl
=
"jdbc:snowflake://myorganization-myaccount.snowflakecomputing.com/"
;
Properties
properties
=
new
Properties
();
properties
.
put
(
"user"
,
"peter"
);
properties
.
put
(
"password"
,
"test"
);
properties
.
put
(
"account"
,
"myorganization-myaccount"
);
properties
.
put
(
"warehouse"
,
"mywh"
);
properties
.
put
(
"db"
,
"mydb"
);
properties
.
put
(
"schema"
,
"public"
);
// get connection
System
.
out
.
println
(
"Create JDBC connection"
);
Connection
connection
=
DriverManager
.
getConnection
(
jdbcUrl
,
properties
);
System
.
out
.
println
(
"Done creating JDBC connection\n"
);
// create statement
System
.
out
.
println
(
"Create JDBC statement"
);
Statement
statement
=
connection
.
createStatement
();
System
.
out
.
println
(
"Done creating JDBC statement\n"
);
// create a table
System
.
out
.
println
(
"Create my_variant_table table"
);
statement
.
executeUpdate
(
"create or replace table my_variant_table(json VARIANT)"
);
statement
.
close
();
System
.
out
.
println
(
"Done creating demo table\n"
);
connection
.
close
();
System
.
out
.
println
(
"Close connection\n"
);
}
}
Copy
Instead of using
SaveMode.Overwrite
, use
SaveMode.Append
, to reuse the existing table. When the string value representing JSON is loaded into Snowflake, because the target column is of type VARIANT, it is parsed as JSON. For example:
df
.
write
.
format
(
SNOWFLAKE_SOURCE_NAME
)
.
options
(
sfOptions
)
.



Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-qubole.txt

Documentation Title:
Configuring Snowflake for Spark in Qubole | Snowflake Documentation

Documentation Content:
Database Type
: Select ‘Snowflake’.
Catalog Name
: Enter the name of the Snowflake catalog.
Database Name
: Enter the name of the database in Snowflake where the data is stored.
Warehouse Name
: Enter the name of the Snowflake virtual warehouse to use for queries.
Host Address
: Enter the base URL of your Snowflake account (e.g.
myorganization-myaccount.snowflakecomputing.com
). See
Account identifiers
for details on
specifying your account identifier in this URL.
Username
: Enter the login name for your Snowflake user (used to connect to the host).
Password
: Enter the password for your Snowflake user (used to connect to the host).
Note that all the values are
case-sensitive
, except for
Host Address
.
Click
Save
to create the data store.
Repeat these steps for each Snowflake database that you want to add as a data store. Or you can edit the data store to change the Snowflake database or any other properties for the data store (e.g.
change the virtual warehouse used for queries).
Note
After adding a Snowflake data store, restart the Spark cluster (if you are using an already-running Spark cluster). Restarting the Spark cluster installs the
.jar
files for the Snowflake
Connector for Spark and the Snowflake JDBC Driver.
Verifying the Snowflake Data Store in Qubole
¶
To verify that the Snowflake data store was created and has been activated, click on the dropdown list in the upper-left of the
Explore
page. A green dot indicates that the data store has
been activated.
You should also verify that the table explorer widget in the left pane of the
Explore
page displays all of the tables in the Snowflake database specified in the data store.
Query Pushdown in Qubole
¶
Spark queries benefit from Snowflake’s automatic query pushdown optimization, which improves performance. By default, Snowflake query pushdown is enabled in Qubole.
For more details about query pushdown, see
Pushing Spark Query Processing to Snowflake
(Snowflake Blog).
Was this page helpful?



Documentation Source:
docs.snowflake.com/en/developer-guide/jdbc/jdbc-using.txt

Documentation Title:
Using the JDBC Driver | Snowflake Documentation

Documentation Content:
close
();
System
.
out
.
println
(
"INFO: Re-opening connection."
);
connection
=
create_connection
(
args
);
use_warehouse_db_and_schema
(
connection
);
resultSet
=
connection
.
unwrap
(
SnowflakeConnection
.
class
).
createResultSet
(
queryID
);
// Assume that the query isn't done yet.
QueryStatus
queryStatus
=
QueryStatus
.
RUNNING
;
while
(
queryStatus
==
QueryStatus
.
RUNNING
)
{
Thread
.
sleep
(
2000
);
// 2000 milliseconds.
queryStatus
=
resultSet
.
unwrap
(
SnowflakeResultSet
.
class
).
getStatus
();
}
if
(
queryStatus
==
QueryStatus
.
FAILED_WITH_ERROR
)
{
System
.
out
.
format
(
"ERROR %d: %s%n"
,
queryStatus
.
getErrorMessage
(),
queryStatus
.
getErrorCode
());
}
else
if
(
queryStatus
!=
QueryStatus
.
SUCCESS
)
{
System
.
out
.
println
(
"ERROR: unexpected QueryStatus: "
+
queryStatus
);
}
else
{
boolean
result_exists
=
resultSet
.
next
();
if
(
!
result_exists
)
{
System
.
out
.
println
(
"ERROR: No rows returned."
);
}
else
{
float
pi_result
=
resultSet
.
getFloat
(
1
);
System
.
out
.
println
(
"pi = "
+
pi_result
);
}
}
Copy
Upload data files directly from a stream to an internal stage
¶
You can upload data files using the PUT command. However, sometimes it makes sense to transfer data directly from a
stream to an internal (i.e. Snowflake) stage as a file. (The
stage
can be any internal stage type: table stage, user stage, or named stage. The JDBC driver does not support uploading to an external
stage.) Here is the method exposed in the
SnowflakeConnection
class:
/**
* Method to compress data from a stream and upload it at a stage location.
* The data will be uploaded as one file. No splitting is done in this method.
*
* Caller is responsible for releasing the inputStream after the method is
* called.



Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-qubole.txt

Documentation Title:
Configuring Snowflake for Spark in Qubole | Snowflake Documentation

Documentation Content:
Configuring Snowflake for Spark in Qubole | Snowflake Documentation
DOCUMENTATION
/
Getting Started
Guides
Developer
Reference
Releases
Tutorials
Status
Overview
Snowpark Library
Snowpark API
Snowpark ML
Snowpark Code Execution Environments
Snowpark Container Services
Functions and Procedures
Snowflake APIs
Snowflake Python API
SQL REST API
Apps
Streamlit in Snowflake
Snowflake Native App Framework
External System Integration
External Functions
Kafka and Spark Connectors
Kafka Connector
Spark Connector
Overview
Install
Configure
Qubole
Usage
Snowflake Scripting
Snowflake Scripting Developer Guide
Tools
Snowflake CLI
Git
Drivers
Overview
Reference
API Reference
Developer
Kafka and Spark Connectors
Spark Connector
Qubole
Configuring Snowflake for Spark in Qubole
¶
To configure Snowflake for Spark in Qubole, you simply add Snowflake as a Qubole data store. This topic provides step-by-step instructions for performing this task using the Qubole Data Service (QDS) UI.
Note
You can also use the QDS REST API to add Snowflake as a data store. For step-by-step instructions, see
Adding a Snowflake Data Warehouse as a Data Store
(in the Qubole Documentation).
Prerequisites
¶
You must be a QDS system administrator to add a data store.
You must have a Qubole Enterprise edition account.
The role used in the connection needs USAGE and CREATE STAGE privileges
on the schema that contains the table that you will read from or write to
via Qubole.
Preparing an External Location for Long-running Queries
¶
If some of your jobs exceed 36 hours in length, consider preparing an
external location to use to exchange data between Snowflake and Spark.
For more information, see
Preparing an External Location For Files
.
Adding Snowflake as a Data Store in the QDS UI
¶
From the
Home
menu, click
Explore
.
In the dropdown list on the
Explore
page, select
+ Add Data Store
.
Enter the required information in the following fields:
Data Store Name
: Enter the name of the data store to be created.



