Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-qubole.html

Documentation Title:
Configuring Snowflake for Spark in Qubole | Snowflake Documentation

Documentation Content:
change the virtual warehouse used for queries).</p><div><p>Note</p><p>After adding a Snowflake data store, restart the Spark cluster (if you are using an already-running Spark cluster). Restarting the Spark cluster installs the <span>.jar</span>files for the Snowflake
Connector for Spark and the Snowflake JDBC Driver.</p></div></section><section><h2>Verifying the Snowflake Data Store in Qubole<a>¶</a></h2><p>To verify that the Snowflake data store was created and has been activated, click on the dropdown list in the upper-left of the <span>Explore</span>page. A green dot indicates that the data store has
been activated.</p><p>You should also verify that the table explorer widget in the left pane of the <span>Explore</span>page displays all of the tables in the Snowflake database specified in the data store.</p></section><section><h2>Query Pushdown in Qubole<a>¶</a></h2><p>Spark queries benefit from Snowflake’s automatic query pushdown optimization, which improves performance. By default, Snowflake query pushdown is enabled in Qubole.</p><p>For more details about query pushdown, see <a>Pushing Spark Query Processing to Snowflake</a>(Snowflake Blog).</p></section></section><footer><div><p>Was this page helpful?</p><button>Yes</button><button>No</button></div><div><a>Visit Snowflake</a><a>Join the conversation</a><a>Develop with Snowflake</a><a>Share your feedback</a><a>Read the latest on our blog</a><a>Get your own certification</a></div><div><a>Privacy Notice</a><a>Site Terms</a><span>© 2024Snowflake, Inc.



Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-qubole.html

Documentation Title:
Configuring Snowflake for Spark in Qubole | Snowflake Documentation

Documentation Content:
For more information, see <span>Preparing an External Location For Files</span>.</p></section><section><h2>Adding Snowflake as a Data Store in the QDS UI<a>¶</a></h2><ol><p>From the <span>Home</span>menu, click <span>Explore</span>.</p><p>In the dropdown list on the <span>Explore</span>page, select <span>+ Add Data Store</span>.</p><li><p>Enter the required information in the following fields:</p><ul><p><span>Data Store Name</span>: Enter the name of the data store to be created.</p><p><span>Database Type</span>: Select ‘Snowflake’.</p><p><span>Catalog Name</span>: Enter the name of the Snowflake catalog.</p><p><span>Database Name</span>: Enter the name of the database in Snowflake where the data is stored.</p><p><span>Warehouse Name</span>: Enter the name of the Snowflake virtual warehouse to use for queries.</p><p><span>Host Address</span>: Enter the base URL of your Snowflake account (e.g.
<span>myorganization-myaccount.snowflakecomputing.com</span>). See <span>Account identifiers</span>for details on
specifying your account identifier in this URL.</p><p><span>Username</span>: Enter the login name for your Snowflake user (used to connect to the host).</p><p><span>Password</span>: Enter the password for your Snowflake user (used to connect to the host).</p></ul><p>Note that all the values are <span>case-sensitive</span>, except for <span>Host Address</span>.</p></li><p>Click <span>Save</span>to create the data store.</p></ol><p>Repeat these steps for each Snowflake database that you want to add as a data store. Or you can edit the data store to change the Snowflake database or any other properties for the data store (e.g.



Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-use.html

Documentation Title:
Using the Spark Connector | Snowflake Documentation

Documentation Content:
</span><span>properties</span><span>.</span><span>put</span><span>(</span><span>"db"</span><span>,</span><span>"mydb"</span><span>);</span><span>properties</span><span>.</span><span>put</span><span>(</span><span>"schema"</span><span>,</span><span>"public"</span><span>);</span><span>// get connection</span><span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Create JDBC connection"</span><span>);</span><span>Connection</span><span>connection</span><span>=</span><span>DriverManager</span><span>.</span><span>getConnection</span><span>(</span><span>jdbcUrl</span><span>,</span><span>properties</span><span>);</span><span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Done creating JDBC connection\n"</span><span>);</span><span>// create statement</span><span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Create JDBC statement"</span><span>);</span><span>Statement</span><span>statement</span><span>=</span><span>connection</span><span>.</span><span>createStatement</span><span>();</span><span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Done creating JDBC statement\n"</span><span>);</span><span>// create a table</span><span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>"Create my_variant_table table"</span><span>);</span><span>statement</span><span>.



Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-qubole.html

Documentation Title:
Configuring Snowflake for Spark in Qubole | Snowflake Documentation

Documentation Content:
This topic provides step-by-step instructions for performing this task using the Qubole Data Service (QDS) UI.</p><div><p>Note</p><p>You can also use the QDS REST API to add Snowflake as a data store. For step-by-step instructions, see
<a>Adding a Snowflake Data Warehouse as a Data Store</a>(in the Qubole Documentation).</p></div><section><h2>Prerequisites<a>¶</a></h2><ul><p>You must be a QDS system administrator to add a data store.</p><p>You must have a Qubole Enterprise edition account.</p><p>The role used in the connection needs USAGE and CREATE STAGE privileges
on the schema that contains the table that you will read from or write to
via Qubole.</p></ul></section><section><h2>Preparing an External Location for Long-running Queries<a>¶</a></h2><p>If some of your jobs exceed 36 hours in length, consider preparing an
external location to use to exchange data between Snowflake and Spark.



