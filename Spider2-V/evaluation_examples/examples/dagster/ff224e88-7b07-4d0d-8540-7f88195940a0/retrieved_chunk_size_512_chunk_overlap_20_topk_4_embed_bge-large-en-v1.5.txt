Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers-legacy.txt

Documentation Title:
IO Managers (Legacy) | Dagster

Documentation Content:
storage_dict
[
(
"123"
,
"abc"
)
]
=
5
context
=
build_input_context
(
upstream_output
=
build_output_context
(
name
=
"abc"
,
step_key
=
"123"
)
)
assert
manager
.
load_input
(
context
)
==
5
Recording metadata from an IO Manager
#
Sometimes, you may want to record some metadata while handling an output in an IO manager. To do this, you can invoke
OutputContext.add_output_metadata
from within the body of the
handle_output
function. Using this, we can modify one of the
above examples
to now include some helpful metadata in the log:
class
DataframeTableIOManagerWithMetadata
(
IOManager
)
:
def
handle_output
(
self
,
context
,
obj
)
:
table_name
=
context
.
name
        write_dataframe_to_table
(
name
=
table_name
,
dataframe
=
obj
)
context
.
add_output_metadata
(
{
"num_rows"
:
len
(
obj
)
,
"table_name"
:
table_name
}
)
def
load_input
(
self
,
context
)
:
table_name
=
context
.
upstream_output
.
name
return
read_dataframe_from_table
(
name
=
table_name
)
Any entries yielded this way will be attached to the
Handled Output
event for this output.
Additionally, if the handled output is part of a software-defined asset, these metadata entries will also be attached to the materialization event created for that asset and show up on the Asset Details page for the asset.
See it in action
#
For more examples of IO Managers, check out the following in our
Hacker News example
:
Parquet IO Manager
Our
Type and Metadata example
also covers writing custom IO managers.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers.txt

Documentation Title:
I/O managers | Dagster

Documentation Content:
ndarray
:
file_path
=
self
.
_get_path
(
context
)
return
load_numpy_array
(
name
=
file_path
)
@asset
(
io_manager_key
=
"pandas_manager"
)
def
upstream_asset
(
)
-
>
pd
.
DataFrame
:
return
pd
.
DataFrame
(
[
1
,
2
,
3
]
)
@asset
(
ins
=
{
"upstream"
:
AssetIn
(
key_prefix
=
"public"
,
input_manager_key
=
"numpy_manager"
)
}
)
def
downstream_asset
(
upstream
:
np
.
ndarray
)
-
>
tuple
:
return
upstream
.
shape


defs
=
Definitions
(
assets
=
[
upstream_asset
,
downstream_asset
]
,
resources
=
{
"pandas_manager"
:
PandasAssetIOManager
(
)
,
"numpy_manager"
:
NumpyAssetIOManager
(
)
,
}
,
)
Testing an I/O manager
#
The easiest way to test an I/O manager is to construct an
OutputContext
or
InputContext
and pass it to the
handle_output
or
load_input
method of the I/O manager. The
build_output_context
and
build_input_context
functions allow for easy construction of these contexts.
Here's an example for a simple I/O manager that stores outputs in an in-memory dictionary that's keyed on the step and name of the output.
from
dagster
import
(
InputContext
,
IOManager
,
OutputContext
,
build_input_context
,
build_output_context
,
)
class
MyIOManager
(
IOManager
)
:
def
__init__
(
self
)
:
self
.
storage_dict
=
{
}
def
handle_output
(
self
,
context
:
OutputContext
,
obj
)
:
self
.
storage_dict
[
(
context
.
step_key
,
context
.
name
)
]
=
obj
def
load_input
(
self
,
context
:
InputContext
)
:
if
context
.
upstream_output
:
return
self
.
storage_dict
[
(
context
.
upstream_output
.
step_key
,
context
.
upstream_output
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers.txt

Documentation Title:
I/O managers | Dagster

Documentation Content:
root_path
+
"/"
.
join
(
asset_key
.
path
)
def
handle_output
(
self
,
context
:
OutputContext
,
obj
)
:
write_csv
(
self
.
_get_path
(
context
.
asset_key
)
,
obj
)
def
load_input
(
self
,
context
:
InputContext
)
:
return
read_csv
(
self
.
_get_path
(
context
.
asset_key
)
)
defs
=
Definitions
(
assets
=
.
.
.
,
resources
=
{
"io_manager"
:
MyIOManager
(
root_path
=
"/tmp/"
)
}
,
)
Handling partitioned assets
#
I/O managers can be written to handle
partitioned
assets. For a partitioned asset, each invocation of
handle_output
will (over)write a single partition, and each invocation of
load_input
will load one or more partitions. When the I/O manager is backed by a filesystem or object store, then each partition will typically correspond to a file or object. When it's backed by a database, then each partition will typically correspond to a range of rows in a table that fall within a particular window.
The default I/O manager has support for loading a partitioned upstream asset for a downstream asset with matching partitions out of the box (see the section below for loading multiple partitions). The
UPathIOManager
can be used to handle partitions in custom filesystem-based I/O managers.
To handle partitions in an custom I/O manager, you'll need to determine which partition you're dealing with when you're storing an output or loading an input. For this,
OutputContext
and
InputContext
have a
asset_partition_key
property:
class
MyPartitionedIOManager
(
IOManager
)
:
def
_get_path
(
self
,
context
)
-
>
str
:
if
context
.
has_partition_key
:
return
"/"
.
join
(
context
.
asset_key
.
path
+
[
context
.
asset_partition_key
]
)
else
:
return
"/"
.
join
(
context
.
asset_key
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers.txt

Documentation Title:
I/O managers | Dagster

Documentation Content:
op_def
.
name
}
doesn't have schema and metadata set"
)
def
load_input
(
self
,
context
:
InputContext
)
:
if
context
.
upstream_output
and
context
.
upstream_output
.
definition_metadata
:
table_name
=
context
.
upstream_output
.
definition_metadata
[
"table"
]
schema
=
context
.
upstream_output
.
definition_metadata
[
"schema"
]
return
read_dataframe_from_table
(
name
=
table_name
,
schema
=
schema
)
else
:
raise
Exception
(
"Upstream output doesn't have schema and metadata set"
)
Per-input loading in assets
#
Let's say you have an asset that is set to store and load as a Pandas DataFrame, but you want to write a new asset that processes the first asset as a NumPy array. Rather than update the I/O manager of the first asset to be able to load as a Pandas DataFrame and a NumPy array, you can write a new loader for the new asset.
In this example, we store
upstream_asset
as a Pandas DataFrame, and we write a new I/O manager to load is as a NumPy array in
downstream_asset
class
PandasAssetIOManager
(
ConfigurableIOManager
)
:
def
handle_output
(
self
,
context
:
OutputContext
,
obj
)
:
file_path
=
self
.
_get_path
(
context
)
store_pandas_dataframe
(
name
=
file_path
,
table
=
obj
)
def
_get_path
(
self
,
context
)
:
return
os
.
path
.
join
(
"storage"
,
f"
{
context
.
asset_key
.
path
[
-
1
]
}
.csv"
,
)
def
load_input
(
self
,
context
:
InputContext
)
-
>
pd
.
DataFrame
:
file_path
=
self
.
_get_path
(
context
)
return
load_pandas_dataframe
(
name
=
file_path
)
class
NumpyAssetIOManager
(
PandasAssetIOManager
)
:
def
load_input
(
self
,
context
:
InputContext
)
-
>
np
.



