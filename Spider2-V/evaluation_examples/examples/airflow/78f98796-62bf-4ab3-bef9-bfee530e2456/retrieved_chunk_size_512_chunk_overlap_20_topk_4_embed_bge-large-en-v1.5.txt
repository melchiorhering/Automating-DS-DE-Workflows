Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
operator_param
%
2
:
return
True
else
:
return
False
You then write a
test_evencheckoperator.py
file with unit tests similar to the following example:
import
unittest
from
datetime
import
datetime
from
airflow
.
models
.
dag
import
DAG
from
airflow
.
models
import
TaskInstance
DEFAULT_DATE
=
datetime
(
2021
,
1
,
1
)
class
EvenNumberCheckOperator
(
unittest
.
TestCase
)
:
def
setUp
(
self
)
:
super
(
)
.
setUp
(
)
self
.
dag
=
DAG
(
"test_dag"
,
default_args
=
{
"owner"
:
"airflow"
,
"start_date"
:
DEFAULT_DATE
}
)
self
.
even
=
10
self
.
odd
=
11
def
test_even
(
self
)
:
"""Tests that the EvenNumberCheckOperator returns True for 10."""
task
=
EvenNumberCheckOperator
(
my_operator_param
=
self
.
even
,
task_id
=
"even"
,
dag
=
self
.
dag
)
ti
=
TaskInstance
(
task
=
task
,
execution_date
=
datetime
.
now
(
)
)
result
=
task
.
execute
(
ti
.
get_template_context
(
)
)
assert
result
is
True
def
test_odd
(
self
)
:
"""Tests that the EvenNumberCheckOperator returns False for 11."""
task
=
EvenNumberCheckOperator
(
my_operator_param
=
self
.
odd
,
task_id
=
"odd"
,
dag
=
self
.
dag
)
ti
=
TaskInstance
(
task
=
task
,
execution_date
=
datetime
.
now
(
)
)
result
=
task
.
execute
(
ti
.
get_template_context
(
)
)
assert
result
is
False
If your DAGs contain
PythonOperators
that execute your own Python functions, it is recommended that you write unit tests for those functions as well.
The most common way to implement unit tests in production is to automate them as part of your CI/CD process. Your CI tool executes the tests and stops the deployment process when errors occur.



Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
tags
,
f"
{
dag_id
}
in
{
fileloc
}
has no tags"
if
APPROVED_TAGS
:
assert
not
set
(
dag
.
tags
)
-
APPROVED_TAGS
tip
You can view the attributes and methods available for the
dag
model in the
Airflow documentation
.
You can also set requirements at the task level by accessing the
tasks
attribute within the
dag
model, which contains a list of all task objects of a DAG. The test below checks that all DAGs contain at least one task and all tasks use
trigger_rule="all_success"
.
@pytest
.
mark
.
parametrize
(
"dag_id,dag,fileloc"
,
get_dags
(
)
,
ids
=
[
x
[
2
]
for
x
in
get_dags
(
)
]
)
def
test_dag_tags
(
dag_id
,
dag
,
fileloc
)
:
"""
test if all DAGs contain a task and all tasks use the trigger_rule all_success
"""
assert
dag
.
tasks
,
f"
{
dag_id
}
in
{
fileloc
}
has no tasks"
for
task
in
dag
.
tasks
:
t_rule
=
task
.
trigger_rule
assert
(
t_rule
==
"all_success"
)
,
f"
{
task
}
in
{
dag_id
}
has the trigger rule
{
t_rule
}
"
Implement DAG validation tests
​
Airflow offers different ways to run DAG validation tests using any Python test runner. This section gives an overview of the most common implementation methods. If you are new to testing Airflow DAGs, you can quickly get started by using Astro CLI commands.
Airflow CLI
​
The Airflow CLI offers two commands related to local testing:
airflow dags test
: Given a DAG ID and execution date, this command writes the results of a single DAG run to the metadata database. This command is useful for testing full DAGs by creating manual DAG runs from the command line.
airflow tasks test
: This command tests one specific task instance without checking for dependencies or recording the outcome in the metadata database.
With the Astro CLI, you can run all Airflow CLI commands using
astro dev run
.



Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
PyCharm
.
Tools like
The Python Debugger
and the built-in
breakpoint()
function. These allow you to run
dag.test()
from the command line by running
python <path-to-dag-file>
.
Use
dag.test()
with the Astro CLI
​
If you use the Astro CLI exclusively and do not have the
airflow
package installed locally, you can still debug using
dag.test()
by running
astro dev start
, entering the scheduler container with
astro dev bash -s
, and executing
python <path-to-dag-file>
from within the Docker container. Unlike using the base
airflow
package, this testing method requires starting up a complete Airflow environment.
Use variables and connections in dag.test()
​
To debug your DAGs in a more realistic environment, you can pass the following Airflow environment configurations to
dag.test()
:
execution_date
passed as a
pendulum.datetime
object.
Airflow connections
passed as a
.yaml
file.
Airflow variables passed as a
.yaml
file.
DAG configuration passed as a dictionary.
This is useful for testing your DAG for different dates or with different connections and configurations. The following code snippet shows the syntax for passing various parameters to
dag.test()
:
from
pendulum
import
datetime
if
__name__
==
"__main__"
:
conn_path
=
"connections.yaml"
variables_path
=
"variables.yaml"
my_conf_var
=
23
dag
.



Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
For example, to run
airflow dags test
on the DAG
my_dag
for the execution date of
2023-01-29
run:
astro dev run dags
test
my_dag
'2023-01-29'
The Astro CLI
​
The Astro CLI includes a suite of commands to help simplify common testing workflows. See
Test your Astro project locally
.
Test DAGs in a CI/CD pipeline
​
You can use CI/CD tools to test and deploy your Airflow code. By installing the Astro CLI into your CI/CD process, you can test your DAGs before deploying them to a production environment. See
set up CI/CD
for example implementations.
info
Astronomer customers can use the Astro GitHub integration, which allows you to automatically deploy code from a GitHUb repository to an Astro deployment, viewing Git metadata in the Astro UI. See
Deploy code with the Astro GitHub integration
.
Add test data or files for local testing
​
Use the
include
folder of your Astro project to store files for testing locally, such as test data or a dbt project file. The files in your
include
folder are included in your deploys to Astro, but they are not parsed by Airflow. Therefore, you don't need to specify them in
.airflowignore
to prevent parsing.
If you're running Airflow locally, apply your changes by refreshing the Airflow UI.
Debug interactively with dag.test()
​
The
dag.test()
method allows you to run all tasks in a DAG within a single serialized Python process, without running the Airflow scheduler. The
dag.test()
method lets you iterate faster and use IDE debugging tools when developing DAGs.
This functionality replaces the deprecated DebugExecutor. Learn more in the
Airflow documentation
.
Prerequisites
​
Ensure that your testing environment has:
Airflow 2.5.0
or later. You can check your version by running
airflow version
.
All provider packages that your DAG uses.
An initialized
Airflow metadata database
, if your DAG uses elements of the metadata database like XCom. The Airflow metadata database is created when Airflow is first run in an environment.



