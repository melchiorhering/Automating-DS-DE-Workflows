Documentation Source:
airbyte.com/tutorials/building-an-e-commerce-data-pipeline-a-hands-on-guide-to-using-airbyte-dbt-dagster-and-bigquery.txt

Documentation Title:
How to build E-commerce Data Pipeline with Airbyte? | Airbyte

Documentation Content:
Create a source:
Go to the Sources tab and click on "+ New source".
Search for ‚Äúfaker‚Äù using the search bar and select "Sample Data (Faker)".
Adjust the Count and optional fields as needed for your use case. You can also leave as is.
Click on "Set up source".
Look fo Faker source connector
Create a Faker source
2. Create a destination:
Go to the Destinations tab and click on "+ New destination".
Search for ‚Äúbigquery‚Äù using the search bar and select "BigQuery".
Enter the connection details as needed.
For simplicity, you can use "Standard Inserts" as the loading method.
In the "Service Account Key JSON" field, enter the contents of the JSON file. Yes, the full JSON.
Click on "Set up destination".
Look for BigQuery destination connector
Create a BigQuery destination
3. Create a connection:
Go to the Connections tab and click on "+ New connection".
Select the source and destination you just created.
Enter the connection details as needed.
For this project, leave the ‚Äúreplication frequency‚Äù as ‚ÄúManual‚Äù, since we will orchestrate the syncs with Dagster.
Click on "Set up connection".
That‚Äôs it! Your connection is set up and ready to go! üéâ
‚Äç
Establish a connector between Faker and BigQuery
4. Setting Up the dbt Project
dbt (data build tool)
allows you to transform your data by writing, documenting, and executing SQL workflows. Setting up the dbt project requires specifying connection details for your data platform, in this case, BigQuery.
1. Navigate to the dbt Project Directory:
Move to the dbt project directory in your project's file structure.
cd ../../dbt_project
This directory contains all the dbt-related configurations and SQL models.
2. Update Connection Details:
Within this directory, you'll find a <span class="text-style-code">profiles.yml file</span>. This file holds the configuration for dbt to connect to BigQuery.



Documentation Source:
airbyte.com/quickstart/airbyte-dbt-and-airflow-stack-with-bigquery.txt

Documentation Title:
E-commerce Analytics Stack with Airbyte, dbt, Airflow (ADA) and BigQuery | Airbyte

Documentation Content:
Search for ‚Äúfaker‚Äù using the search bar and select "Sample Data (Faker)".
Adjust the count and optional fields as needed for your use case. You can also leave as is.
Click on "Set up source".
2. Create a destination
:
Go to the "Destinations" tab and click on "+ New destination".
Search for ‚Äúbigquery‚Äù using the search bar and select BigQuery.
Enter the connection details as needed.
For simplicity, you can use "Standard Inserts" as the loading method.
In the Service Account Key JSON field, enter the contents of the JSON file. Yes, the full JSON.
Click on Set up destination.
3. Create a connection
:
Go to the "Connections" tab and click on "+ New connection".
Select the source and destination you just created.
Enter the connection details as needed.
Click on "Set up connection".
That‚Äôs it! Your connection is set up and ready to go! üéâ
4. Setting Up the dbt Project
dbt (data build tool)
allows you to transform your data by writing, documenting, and executing SQL workflows. Setting up the dbt project requires specifying connection details for your data platform, in this case, BigQuery. Here‚Äôs a step-by-step guide to help you set this up:
1. Navigate to the dbt Project Directory
:
Move to the directory containing the dbt configuration:
cd ../../dbt_project
‚Äç
2. Update Connection Details
:
You'll find a <span class="text-style-code">profiles.yml</span> file within the directory. This file contains configurations for dbt to connect with your data platform.
Update this file with your BigQuery connection details. Specifically, you need to update the Service Account JSON file path, the dataset location and your BigQuery project ID.
Provide your BigQuery project ID in the database field of the <span class="text-style-code">/models/ecommerce/sources/faker_sources.yml</span> file.
3. Test the Connection (Optional)
:
You can test the connection to your BigQuery instance using the following command.



Documentation Source:
airbyte.com/tutorials/elt-pipeline-prefect-airbyte-dbt.txt

Documentation Title:
Orchestrate ELT pipelines with Prefect, Airbyte and dbt | Airbyte

Documentation Content:
In this recipe we‚Äôll create a Prefect flow to orchestrate Airbyte and dbt.
Airbyte
is a data integration tool that allows you to extract data from APIs and databases and load it to data warehouses, data lakes, and databases. In this recipe, we‚Äôll use Airbyte to replicate data from the GitHub API into a Snowflake warehouse.
dbt
is a data transformation tool that allows you to transform data within a data warehouse more effectively. We‚Äôll use dbt in this recipe to transform data from multiple sources into one table to find common contributors between our three repositories.
Prerequisites
In order to follow the steps in this recipe you‚Äôll need to make sure that you have the following installed on your local machine:
Docker and Docker Compose
Python 3.8+
Set up Snowflake
For this tutorial, you can set up a Snowflake account if you don‚Äôt already have one. Snowflake has a generous free tier, so no cost will be incurred while going through this recipe. Airbyte requires some resources to be created in Snowflake to enable data replication from GitHub. You can refer to the
Airbyte Snowflake destination documentation
for the steps necessary to configure Snowflake to allow Airbyte to load data in.
Should you build or buy your data pipelines?
Download our free guide and discover the best approach for your needs, whether it's building your ELT solution in-house or opting for Airbyte Open Source or Airbyte Cloud.
Download now
Set up Airbyte
For this recipe, we‚Äôll use Docker Compose to run an Airbyte installation locally. The commands necessary to run Airbyte can be found in the
Airbyte quickstart guide
.
Set up GitHub sources
Airbyte has a GitHub source that allows us to easily pull the information that we want via the GitHub API. We will set up a source for each of the three repositories that we want to pull data from so that we can have tables in Snowflake for each repository. This is the configuration that we‚Äôll be using for the Prefect repository:
‚Äç
Once that source is successfully configured, we‚Äôll set up two additional sources for the airbytehq/airbyte and dbt-labs/dbt-core repositories.



Documentation Source:
airbyte.com/quickstart/airbyte-dbt-and-airflow-stack-with-snowflake.txt

Documentation Title:
Airbyte, dbt and Airflow (ADA) Stack with Snowflake | Airbyte

Documentation Content:
Particularly, modify the
AIRFLOW_AIRBYTE_CONN
value which is the connection URI that Airflow uses to connect to the Airbyte API. See
here
for more details.
Also modify the
AIRBYTE_CONN_ID
value which is the id of the connection you have set up in Airbyte.
3. Build and Run Airflow Locally
:
Build our Airflow image with the necessary packages and services:
docker compose build
And then run it:
docker compose up
4. Access Airflow in Your Browser
:
When it's done, you can access the Airflow UI at
http://127.0.0.1:8080
. The default username and password are both airflow, unless you changed it on the .env file.
Here, you should see the DAG for the Extract, Load and Transformation pipeline. To get an overview of DAG, click on the DAG's name and select the Graph view. This will give you a clear picture of the process lineage and visualize how the operation flows from extraction to transformation.
Next Steps
Once you've set up and launched this initial integration, the real power lies in its adaptability and extensibility. Here‚Äôs a roadmap to help you customize and harness this project tailored to your specific data needs:
1. Create dbt Sources for Airbyte Data
:
Your raw data extracted via Airbyte can be represented as sources in dbt. Start by
creating new dbt sources
to represent this data, allowing for structured transformations down the line.
2. Add Your dbt Transformations
:
With your dbt sources in place, you can now build upon them. Add your custom SQL transformations in dbt, ensuring that you treat the sources as an upstream dependency. This ensures that your transformations work on the most up-to-date raw data.
3. Execute the Pipeline in Airflow
:
Navigate to the Airflow UI and Trigger the DAG. This triggers the entire pipeline, encompassing the extraction via Airbyte, transformations via dbt, and any other subsequent steps. Modify the schedule as well to suit your use case.
4. Extend the Project
:
The real beauty of this integration is its extensibility.



