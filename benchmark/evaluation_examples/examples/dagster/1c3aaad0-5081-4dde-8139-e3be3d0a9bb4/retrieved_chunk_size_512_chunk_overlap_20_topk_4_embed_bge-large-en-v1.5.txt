Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/partitions-schedules-sensors/sensors.txt

Documentation Title:
Sensors | Dagster Docs

Documentation Content:
Click
Overview > Sensors
.
Click the sensor you want to test.
Click the
Test Sensor
button, located near the top right corner of the page.
You'll be prompted to provide a cursor value. You can use the existing cursor for the sensor (which will be prepopulated) or enter a different value. If you're not using cursors, leave this field blank.
Click
Evaluate
to fire the sensor. A window containing the result of the evaluation will display, whether it's run requests, a skip reason, or a Python error:
If the run was successful, then for each produced run request, you can open the launchpad pre-scaffolded with the config produced by that run request. You'll also see a new computed cursor value from the evaluation, with the option to persist the value.
Via the CLI
#
To quickly preview what an existing sensor will generate when evaluated, run the following::
dagster sensor preview my_sensor_name
Via Python
#
To unit test sensors, you can directly invoke the sensor's Python function. This will return all the run requests yielded by the sensor. The config obtained from the returned run requests can be validated using the
validate_run_config
function:
from
dagster
import
validate_run_config
@sensor
(
job
=
log_file_job
)
def
sensor_to_test
(
)
:
yield
RunRequest
(
run_key
=
"foo"
,
run_config
=
{
"ops"
:
{
"process_file"
:
{
"config"
:
{
"filename"
:
"foo"
}
}
}
}
,
)
def
test_sensor
(
)
:
for
run_request
in
sensor_to_test
(
)
:
assert
validate_run_config
(
log_file_job
,
run_request
.
run_config
)
Notice that since the context argument wasn't used in the sensor, a context object doesn't have to be provided. However, if the context object
is
needed, it can be provided via
build_sensor_context
. Consider again the
my_directory_sensor_cursor
example:
@sensor
(
job
=
log_file_job
)
def
my_directory_sensor_cursor
(
context
)
:
last_mtime
=
float
(
context
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/partitions-schedules-sensors/sensors.txt

Documentation Title:
Sensors | Dagster Docs

Documentation Content:
from
dagster
import
build_sensor_context
,
validate_run_config
def
test_process_new_users_sensor
(
)
:
class
FakeUsersAPI
:
def
fetch_users
(
self
)
-
>
List
[
str
]
:
return
[
"1"
,
"2"
,
"3"
]
context
=
build_sensor_context
(
)
run_requests
=
process_new_users_sensor
(
context
,
users_api
=
FakeUsersAPI
(
)
)
assert
len
(
run_requests
)
==
3
Monitoring sensors in the Dagster UI
#
Using the UI, you can monitor and operate sensors. The UI provides multiple views that help with observing sensor evaluations, skip reasons, and errors.
To view all sensors, navigate to
Overview > Sensors
. Here, you can start and stop sensors, and view their frequency, last tick, and last run:
Click on any sensor to
test the sensor
, monitor all sensor evaluations on a timeline, and view a table of runs launched by the sensor.
Run status sensors
#
If you want to act on the status of a job run, Dagster provides a way to create a sensor that reacts to run statuses. You can use
run_status_sensor
with a specified
DagsterRunStatus
to decorate a function that will run when the given status occurs. This can be used to launch runs of other jobs, send alerts to a monitoring service on run failure, or report a run success.
Here is an example of a run status sensor that launches a run of
status_reporting_job
if a run is successful:
@run_status_sensor
(
run_status
=
DagsterRunStatus
.
SUCCESS
,
request_job
=
status_reporting_job
,
)
def
report_status_sensor
(
context
)
:
# this condition prevents the sensor from triggering status_reporting_job again after it succeeds
if
context
.
dagster_run
.
job_name
!=
status_reporting_job
.
name
:
run_config
=
{
"ops"
:
{
"status_report"
:
{
"config"
:
{
"job_name"
:
context
.
dagster_run
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/partitions-schedules-sensors/sensors.txt

Documentation Title:
Sensors | Dagster Docs

Documentation Content:
json
(
)
@job
def
process_user
(
)
:
.
.
.
@sensor
(
job
=
process_user
)
def
process_new_users_sensor
(
context
:
SensorEvaluationContext
,
users_api
:
UsersAPI
,
)
:
last_user
=
int
(
context
.
cursor
)
if
context
.
cursor
else
0
users
=
users_api
.
fetch_users
(
)
num_users
=
len
(
users
)
for
user_id
in
users
[
last_user
:
]
:
yield
RunRequest
(
run_key
=
user_id
,
tags
=
{
"user_id"
:
user_id
}
,
)
context
.
update_cursor
(
str
(
num_users
)
)
defs
=
Definitions
(
jobs
=
[
process_user
]
,
sensors
=
[
process_new_users_sensor
]
,
resources
=
{
"users_api"
:
UsersAPI
(
url
=
"https://my-api.com/users"
)
}
,
)
For more information on resources, refer to the
Resources documentation
. To see how to test schedules with resources, refer to the section on
Testing sensors with resources
.
Logging in sensors
#
Sensor logs are stored in your
Dagster instance's compute log storage
. You should ensure that your compute log storage is configured to view your sensor logs.
Any sensor can emit log messages during its evaluation function:
@sensor
(
job
=
the_job
)
def
logs_then_skips
(
context
)
:
context
.
log
.
info
(
"Logging from a sensor!"
)
return
SkipReason
(
"Nothing to do"
)
These logs can be viewed when inspecting a tick in the tick history view on the corresponding sensor page.
Testing sensors
#
Via the Dagster UI
Via the CLI
Via Python
Via the Dagster UI
#
Before you test
: Test evaluations of sensors run the sensor's underlying Python function, meaning that any side effects contained within that sensor's function may be executed.
In the UI, you can manually trigger a test evaluation of a sensor and view the results.
Click
Overview > Sensors
.
Click the sensor you want to test.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/partitions-schedules-sensors/sensors.txt

Documentation Title:
Sensors | Dagster Docs

Documentation Content:
join
(
MY_DIRECTORY
,
filename
)
if
os
.
path
.
isfile
(
filepath
)
:
yield
RunRequest
(
run_key
=
filename
,
run_config
=
RunConfig
(
ops
=
{
"process_file"
:
FileConfig
(
filename
=
filename
)
}
)
,
)
This sensor iterates through all the files in
MY_DIRECTORY
and yields a
RunRequest
for each file. Note that despite the
yield
syntax, the function will run to completion before any runs are submitted.
To write a sensor that materializes assets, you can
build a job that materializes assets
:
asset_job
=
define_asset_job
(
"asset_job"
,
"*"
)
@sensor
(
job
=
asset_job
)
def
materializes_asset_sensor
(
)
:
yield
RunRequest
(
.
.
.
)
Once a sensor is added to a
Definitions
object with the job it yields a
RunRequest
for, it can be started and will start creating runs. You can start or stop sensors in the Dagster UI, or by setting the default status to
DefaultSensorStatus.RUNNING
in code:
@sensor
(
job
=
asset_job
,
default_status
=
DefaultSensorStatus
.
RUNNING
)
def
my_running_sensor
(
)
:
.
.
.
If you manually start or stop a sensor in the UI, that will override any default status that is set in code.
Once your sensor is started, if you're running a
Dagster daemon
as part of your deployment, the sensor will begin executing immediately without needing to restart the dagster-daemon process.
Idempotence and cursors
#
When instigating runs based on external events, you usually want to run exactly one job run for each event.



