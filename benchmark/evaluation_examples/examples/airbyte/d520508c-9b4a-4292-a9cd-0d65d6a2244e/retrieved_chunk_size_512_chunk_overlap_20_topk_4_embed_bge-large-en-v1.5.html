Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.html

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
After specifying the Destination Path, click on Set up destination.¬†</p><figcaption>Configure the Local JSON destination</figcaption><p>‚Äç</p><p>This will take you to a page to set up the connection. Set the replication frequency to <strong>Manual</strong>(since we will use Airflow to trigger Airbyte syncs rather than using Airbyte‚Äôs scheduler) and then click on <strong>Set up connection</strong>as highlighted in the image below.</p><figcaption>Specify connection settings</figcaption><p>‚Äç</p><p>Trigger a sync from the <strong>Sample Data (faker)</strong>source to the <strong>Local JSON</strong>output by clicking on <strong>Sync now</strong>as highlighted in the image below.</p><figcaption>Manually trigger a sync from the UI</figcaption><p>‚Äç</p><p>The sync should take a few seconds, at which point you should see that the sync has succeed as shown below.</p><figcaption>After the sync has completed</figcaption><p>‚Äç</p><p>You can now confirm if some sample data has been copied to the expected location. As previously mentioned, for this example the JSON data can be seen in <strong>/tmp/airbyte_local_json_from_faker</strong>. Because there were three streams generated, the following three JSON files should be available:¬†</p><code>_airbyte_raw_products.jsonl	
_airbyte_raw_users.jsonl
_airbyte_raw_purchases.jsonl
</code><p>You have now created a simple example connection in Airbyte which can be manually triggered. A manually triggered connection is ideal for situations where you wish to use an external orchestrator.¬†</p><p>In the next section you will see how to trigger a manual sync on this connection by hitting a REST endpoint directly. After that, you will see how Airflow can be used to hit that same endpoint to trigger synchronizations.¬†</p><h2>Test the API endpoints with cURL</h2><p>Before using the REST endpoint from within Airflow, it is useful to verify that it is working as expected.



Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.html

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
For this tutorial I use the following default values:¬†</p><code>BASIC_AUTH_USERNAME=airbyte
BASIC_AUTH_PASSWORD=password
</code><p>Once Airbyte is running, in your browser type in localhost:8000, which should prompt you for a username and password as follows:</p><figcaption>Airbyte OSS¬†login prompt</figcaption><h2>Create a connection</h2><p>Create a connection that sends data from the <strong>Sample Data (Faker)</strong>source to the <strong>Local JSON</strong>(file system) output. Click on ‚ÄúCreate your first connection‚Äù as shown below:</p><figcaption>Create your first connection prompt</figcaption><p>‚Äç</p><p>You should then see an option to set up a source connection. Select the Faker source from the dropdown as shown below.</p><figcaption>Select Sample Data (Faker) as a source</figcaption><p>‚Äç</p><p>After selecting Sample Data as the source, you will see a screen that should look as follows. Click on <strong>Set up source</strong>as shown below.¬†</p><figcaption>Configure Sample Data (Faker) as a source</figcaption><p>‚Äç</p><p>You will then wait a few seconds for the Sample Data source to be verified, at which point you will be prompted to configure the destination that will be used for the connection. Select <strong>Local JSON</strong>as shown below:</p><figcaption>Select Local JSON as a destination</figcaption><p>‚Äç</p><p>After selecting Local JSON as the output, you will need to specify where the JSON files should be written. By default the path that you specify will be located inside <strong>/tmp/airbyte_local</strong>. In this tutorial I set the destination to <strong>/json_from_faker</strong>, which means that the data will be copied to<strong>/tmp/airbyte_local/json_from_faker</strong>on the localhost where Airbyte is running. After specifying the Destination Path, click on Set up destination.



Documentation Source:
airbyte.com/quickstart/airbyte-dbt-and-airflow-stack-with-bigquery.html

Documentation Title:
E-commerce Analytics Stack with Airbyte, dbt, Airflow (ADA) and BigQuery | Airbyte

Documentation Content:
Link Airbyte connection to the Airflow DAG</strong>:</h3><p>The last step being being able to execute the DAG in Airflow, is to include the connection ID from Airbyte:</p><ol><li>Visit the Airbyte UI at <a>http://localhost:8000/</a>.</li><li>In the "Connections" tab, select the "Faker to BigQuery" connection and copy its connection id from the URL.</li><li>Update the &lt;span class="text-style-code"&gt;connection_id&lt;/span&gt; in the &lt;span class="text-style-code"&gt;extract_data&lt;/span&gt; task within &lt;span class="text-style-code"&gt;orchestration/airflow/dags/elt_dag.py&lt;/span&gt; with this id.</li></ol><p>That's it! Airflow has been configured to work with dbt and Airbyte. üéâ</p><h2>6. Orchestrating with Airflow</h2><p>Now that everything is set up, it's time to run your data pipeline!</p><ol><li>In the Airflow UI, go to the "DAGs" section.</li><li>Locate &lt;span class="text-style-code"&gt;elt_dag&lt;/span&gt; and click on "Trigger DAG" under the "Actions" column.</li></ol><p>This will initiate the complete data pipeline, starting with the Airbyte sync from Faker to BigQuery, followed by dbt transforming the raw data into staging and marts models.



Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.html

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
</p><p>To define a connection Airflow will use to communicate with Airbyte, go to <strong>admin‚Üíconnections</strong>as shown below:</p><p>‚Äç</p><figcaption>Create an Airflow connection</figcaption><p>‚Äç</p><p>Then click in the<strong>+</strong>symbol as annotated in the image below:</p><figcaption>Click on the button to create a new Airflow connection</figcaption><p>‚Äç</p><p>Complete the information about the connection which Airflow will use to connect to Airbyte as follows, and click on the <strong>Test</strong>button. This should look as follows:¬†</p><figcaption>Configure an Airflow connection to Airbyte</figcaption><p>The connection parameters are:</p><ul><li><strong>Connection Id:</strong>Define an identifier that Airflow DAGs can use to communicate with Airbyte. In this example the identifier is given the name <strong>airflow-call-to-airbyte-example</strong>, which will be used in the DAG definition (shown later).¬†</li><li><strong>Connection Type</strong>: Specifies that this is a connection to Airbyte. Note that if you do not see <strong>Airbyte</strong>in the dropdown menu, then the Docker image has not been correctly built. Adding the <a>Airbyte provider</a>to the Docker image was done earlier in this tutorial.¬†</li><li><strong>Host</strong>: The host which is running Airbyte. Note the use of <strong>host.docker.internal</strong>, which¬† resolves to the internal IP address used by the host, as discussed in <a>Docker‚Äôs instructions on network interfaces</a>.</li><li><strong>Login</strong>: The default user to connect to Airbyte is <strong>airbyte</strong>. If you have changed this, then use whichever username you have defined.</li><li><strong>Password</strong>: If you are using the default then the value is <strong>password</strong>.



