Documentation Source:
airbyte.com/tutorials/mysql-change-data-capture-cdc.txt

Documentation Title:
MySQL CDC: Build an ELT pipeline from MySQL Database | Airbyte

Documentation Content:
Step 5: Create an Airbyte connection
Go to connections and create a new connection. Then, select the existing MySQL source you have just created and then do the same for the Local JSON destination. Once you're done, you can set up the connection as follows.
As you can see, I set the replication frequency to
manual
so I can trigger synchronization on demand. You can change the replication frequency, later on, to sync
as frequently as every 5 minutes
.
Then, it's time to configure the
streams
, which in this case are the tables in your database. For now, you only have the
cars
table. If you expand it, you can see the columns it has.
Now, you should select a sync mode. If you want to take full advantage of performing MySQL CDC, you should use
Incremental | Append
mode to only look at the rows that have changed in the source and sync them to the destination. Selecting a
Full Refresh
mode would sync the whole source table, which is most likely not what you want when using CDC. Learn more about sync modes in
our documentation
.
When using an
Incremental
sync mode, you would generally need to provide a
Cursor field
, but when using CDC, that's not necessary since the changes in the source are detected via the Debezium connector stream.
Once you're ready, save the changes. Then, you can run your first sync by clicking on
Sync now
. You can check your run logs to verify everything is going well. Just wait for the sync to be completed, and that's it! You've replicated data from MySQL using CDC.
Step 6: Verify that the sync worked
From the root directory of the Airbyte project, go to
<span class="text-style-code-dark">tmp/airbyte_local/json_data/</span>
, and you will find a file named
<span class="text-style-code-dark">_airbyte_raw_cars.jsonl</span>
where the data from the MySQL database was replicated.
You can check the file's contents in your preferred IDE or run the following command.



Documentation Source:
airbyte.com/tutorials/mysql-change-data-capture-cdc.txt

Documentation Title:
MySQL CDC: Build an ELT pipeline from MySQL Database | Airbyte

Documentation Content:
Although the database can be accessed with the root user, it is advisable to use a less privileged read-only user to read data. The user will be called
airbyte
and the password should be updated with a strong password of your choice.
CREATE USER 'airbyte'@'%' IDENTIFIED BY 'password';
‍
For the
CDC replication method
, you need to grant SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, and REPLICATION CLIENT permissions to the user.
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'airbyte'@'%';
‍
That’s it! Your database in MySQL is ready to be used.
Should you build or buy your data pipelines?
Download our free guide and discover the best approach for your needs, whether it's building your ELT solution in-house or opting for Airbyte Open Source or Airbyte Cloud.
Download now
Step 3: Configure a MySQL source in Airbyte
To set up a new MySQL Airbyte source, go to Airbyte's UI at
localhost:8000
, click on sources and add a new source. As the connector type, select
MySQL
. As demonstrated in the subsequent illustrations, fill in the following configuration fields if you used the instructions above to configure your  database in MySQL.
‍
‍
Then click on
Set up source
and Airbyte will test the connection. If everything goes well, you should see a successful message.
Step 4: Configure a local JSON destination in Airbyte
Now, you’ll configure a local JSON destination in Airbyte. Take into account that I use local JSON as a destination for demonstration purposes – as it’s the easiest to set up. For your actual applications, you can select any destination from our
ever-growing catalog
.
Go to destinations and add a new one. As demonstrated in the following diagram, select
Local JSON
as the destination type and fill in with the following details.
‍
Then click on
Set up source
and let Airbyte test the destination.
Step 5: Create an Airbyte connection
Go to connections and create a new connection.



Documentation Source:
airbyte.com/docs.airbyte.com/using-airbyte/getting-started/set-up-a-connection.txt

Documentation Title:
Set up a Connection | Airbyte Documentation

Documentation Content:
Next, you can toggle which streams you want to replicate. Our test data consists of three streams, which we've enabled and set to
Incremental - Append + Deduped
sync mode.
Your sync mode is already determined by your selection above, but you can change the sync mode for an individual stream. You can also select a cursor or primary key to enable incremental and/or deduplication. For more information on the nature of each sync mode supported by Airbyte, see
this page
.
You can also select individual fields to sync on this page. Expand the fields available by clicking any stream. This is helpful when you have security concerns or don't want to sync all the data from the source.
Click
Next
to complete your stream setup and move to the connection configuration. This is where you'll set up how often your data will sync and where it will live in the destination. For this demo, we'll set the connection to run at 8 AM every day and sync the connection to a custom namespace with a stream prefix.
note
To ensure your data is synced to the correct place, see our examples for
Destination Namespace
Once you've set up all the connection settings, click "Set up connection". You've successfully set up your first data pipeline with Airbyte. Your first sync is about to begin!
Connection Overview
​
Once you've finished setting up the connection, you will be automatically redirected to a connection overview containing all the tools you need to keep track of your connection.
Here's a basic overview of the tabs and their use:
The
Status
tab shows you an overview of your connector's sync health.
The
Job History
tab allows you to check the logs for each sync. If you encounter any errors or unexpected behaviors during a sync, checking the logs is always a good first step to finding the cause and solution.
The
Schema
tab allows you to modify the streams you chose during the connection setup.
The
Transformation
tab allows you to set up a custom post-sync transformations using dbt.
The
Settings
tab contains the connection settings, and the option to delete the connection if you no longer wish to use it.



Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.txt

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
By default the path that you specify will be located inside
/tmp/airbyte_local
. In this tutorial I set the destination to
/json_from_faker
, which means that the data will be copied to
/tmp/airbyte_local/json_from_faker
on the localhost where Airbyte is running. After specifying the Destination Path, click on Set up destination.
Configure the Local JSON destination
‍
This will take you to a page to set up the connection. Set the replication frequency to
Manual
(since we will use Airflow to trigger Airbyte syncs rather than using Airbyte’s scheduler) and then click on
Set up connection
as highlighted in the image below.
Specify connection settings
‍
Trigger a sync from the
Sample Data (faker)
source to the
Local JSON
output by clicking on
Sync now
as highlighted in the image below.
Manually trigger a sync from the UI
‍
The sync should take a few seconds, at which point you should see that the sync has succeed as shown below.
After the sync has completed
‍
You can now confirm if some sample data has been copied to the expected location. As previously mentioned, for this example the JSON data can be seen in
/tmp/airbyte_local_json_from_faker
. Because there were three streams generated, the following three JSON files should be available:
_airbyte_raw_products.jsonl	
_airbyte_raw_users.jsonl
_airbyte_raw_purchases.jsonl
You have now created a simple example connection in Airbyte which can be manually triggered. A manually triggered connection is ideal for situations where you wish to use an external orchestrator.
In the next section you will see how to trigger a manual sync on this connection by hitting a REST endpoint directly. After that, you will see how Airflow can be used to hit that same endpoint to trigger synchronizations.
Test the API endpoints with cURL
Before using the REST endpoint from within Airflow, it is useful to verify that it is working as expected.



