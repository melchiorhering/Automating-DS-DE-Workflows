Documentation Source:
airbyte.com/docs.airbyte.com/integrations/destinations/snowflake.txt

Documentation Title:
Snowflake | Airbyte Documentation

Documentation Content:
When creating a connection, it may allow you to select
Mirror source structure
for the
Destination namespace
, which if you have followed some of our default examples and tutorials may
result in the connection trying to write to a
PUBLIC
schema.
A quick fix could be to edit your connection's 'Replication' settings from
Mirror source structure
to
Destination Default
. Otherwise, make sure to grant the role the required permissions in the
desired namespace.
Changelog
​
Version
Date
Pull Request
Subject
3.8.0
2024-05-08
#37715
Remove option for incremental typing and deduping
3.7.4
2024-05-07
#38052
Revert problematic optimization
3.7.3
2024-05-07
#34612
Adopt CDK 0.33.2
3.7.2
2024-05-06
#37857
Use safe executeMetadata call
3.7.1
2024-04-30
#36910
Bump CDK version
3.7.0
2024-04-08
#35754
Allow configuring
data_retention_time_in_days
; apply to both raw and final tables.
Note
: Existing tables will not be affected; you must manually alter them.



Documentation Source:
docs.snowflake.com/en/user-guide/spark-connector-overview.txt

Documentation Title:
Overview of the Spark Connector | Snowflake Documentation

Documentation Content:
Also, the files created by the connector during external transfer are intended to be temporary, but the connector does not automatically delete the files from the storage location. To delete the
files, use any of the following methods:
Delete them manually.
Set the
purge
parameter for the connector. For more information about this parameter, see
Setting Configuration Options for the Connector
.
Set a storage system parameter, such as the Amazon S3 lifecycle policy parameter, to clean up the files after the transfer is done.
Column Mapping
¶
When you copy data from a Spark table to a Snowflake table, if the column names do not match, you can map column names from Spark to Snowflake using the
columnmapping
parameter, which is
documented in
Setting Configuration Options for the Connector
.
Note
Column mapping is supported only for internal data transfer.
Query Pushdown
¶
For optimal performance, you typically want to avoid reading lots of data or transferring large intermediate results between systems. Ideally, most of the processing should happen close to where
the data is stored to leverage the capabilities of the participating stores to dynamically eliminate data that is not needed.
Query pushdown leverages these performance efficiencies by enabling large and complex Spark logical plans (in their entirety or
in parts) to be processed in Snowflake, thus using Snowflake to do most of the actual work.
Query pushdown is supported in Version 2.1.0 (and higher) of the Snowflake Connector for Spark.
Pushdown is not possible in all situations. For example, Spark UDFs cannot be pushed down to Snowflake. See
Pushdown
for the list of operations supported for pushdown.
Note
If you need pushdown for all operations, consider writing your code to use
Snowpark API
instead.
Snowpark also supports pushdown of Snowflake UDFs.
Databricks Integration
¶
Databricks has integrated the Snowflake Connector for Spark into the Databricks Unified Analytics Platform to provide native connectivity between Spark and Snowflake.
For more details, including code examples using Scala and Python, see
Data Sources — Snowflake
(in the Databricks documentation) or
Configuring Snowflake for Spark in Databricks
.



Documentation Source:
docs.snowflake.com/en/user-guide/cleanrooms/tutorials/cleanroom-web-app-tutorial.txt

Documentation Title:
Tutorial: Get started with the web app of a Snowflake Data Clean Room | Snowflake Documentation

Documentation Content:
Open
Projects
»
Worksheets
.
Select
+
»
SQL Worksheet
.
In the new worksheet, paste and run the following statement to list the activation data that was pushed from the consumer’s
clean room environment.
SELECT
*
FROM
samooha_by_snowflake_local_db
.
public
.
provider_activation_summary
WHERE
segment
=
'Provider Snowflake Account'
;
Copy
Clean up
¶
Supported Regions for Feature
Available in select regions on Amazon Web Services and Microsoft Azure. See
Available regions
.
You can delete the clean room and activation data that you created for this tutorial to clean up your production environment.
Delete the activation data
¶
To delete the activation data from the provider’s Snowflake account:
Sign in to Snowsight for the provider account. You are signing in to the Snowflake account, not the clean room environment.
Open
Projects
»
Worksheets
.
Select
+
»
SQL Worksheet
.
In the new worksheet, paste and run the following statement to delete the activation data created for this tutorial:
DELETE
FROM
samooha_by_snowflake_local_db
.
public
.
provider_activation_summary
WHERE
segment
=
'Provider Snowflake Account'
;
Copy
Delete the clean room
¶
Deleting a clean room in the provider account removes it from both the provider account and the consumer account.
To delete a clean room:
Navigate to the
sign in page
.
Enter your email address, and select
Continue
.
Enter your password.
Select the Snowflake account that you used as the provider account.
In the left navigation, select
Clean Rooms
.
On the
Created
tab, find the
Tutorial
tile and select the More icon (
).
Select
Delete
.
Select
Proceed
.
Learn more
¶
Supported Regions for Feature
Available in select regions on Amazon Web Services and Microsoft Azure. See
Available regions
.
Congratulations! You have now used the web app to create and share a clean room as a provider. You have also acted as the consumer
who is using the clean room to analyze data within a privacy-preserving environment.



Documentation Source:
docs.snowflake.com/en/user-guide/tutorials/data-load-internal-tutorial.txt

Documentation Title:
Tutorial: Bulk loading from a local file system using COPY | Snowflake Documentation

Documentation Content:
|
|       "last": "Ramos"                                           |
|     },                                                          |
|     "phone": "+1 (962) 436-2519"                                |
|   }                                                             |
| }                                                               |
+
-----------------------------------------------------------------+
Copy
Step 6. Remove the successfully copied data files
¶
After you verify that you successfully copied data from your stage into the tables,
you can remove data files from the internal stage using the
REMOVE
command to save on
data storage
.
REMOVE
@
my_csv_stage
PATTERN
=
'.*.csv.gz'
;
Copy
Snowflake returns the following results:
+
-------------------------------+---------+
| name                          | result  |
|-------------------------------+---------|
| my_csv_stage/contacts1.csv.gz | removed |
| my_csv_stage/contacts4.csv.gz | removed |
| my_csv_stage/contacts2.csv.gz | removed |
| my_csv_stage/contacts3.csv.gz | removed |
| my_csv_stage/contacts5.csv.gz | removed |
+
-------------------------------+---------+
Copy
REMOVE
@
my_json_stage
PATTERN
=
'.*.json.gz'
;
Copy
Snowflake returns the following results:
+
--------------------------------+---------+
| name                           | result  |
|--------------------------------+---------|
| my_json_stage/contacts.json.gz | removed |
+
--------------------------------+---------+
Copy
Step 7. Clean up
¶
Congratulations, you have successfully completed the tutorial.
Tutorial clean up (optional)
¶
Execute the following
DROP <object>
commands to return your system to its state before you began the tutorial:
DROP
DATABASE
IF
EXISTS
mydatabase
;
DROP
WAREHOUSE
IF
EXISTS
mywarehouse
;
Copy
Dropping the database automatically removes all child database objects such as tables.
Other data loading tutorials
¶
Snowflake in 20 minutes
Tutorial: Bulk loading from Amazon S3 using COPY
Was this page helpful?
Yes
No
Visit Snowflake
Join the conversation
Develop with Snowflake
Share your feedback
Read the latest on our blog
Get your own certification
Privacy Notice
Site Terms
©
2024
Snowflake, Inc.
All Rights Reserved
.



