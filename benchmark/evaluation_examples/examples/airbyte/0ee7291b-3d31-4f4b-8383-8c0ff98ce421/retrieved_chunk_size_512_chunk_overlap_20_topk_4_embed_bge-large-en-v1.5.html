Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.html

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
</p><p>For Snowflake, your connection string will follow the following format:</p><code>"snowflake://[username]:[password]@[account]/[DATABASE]/[SCHEMA]?warehouse=[WAREHOUSE]&amp;role=[ROLE]"
</code><p>Because the table I replicated into Snowflake is in the `development` database and `customers` schema, I’ll input this information into the string. I’ll also be using `external_user` for my user, the `validator_wh` for my warehouse, and the `validator’ role here. These are different from what I used in the loading process since this is a different step in the data pipeline. </p><p>Make sure whatever role you are using has the appropriate permissions to access the table you are comparing! The user you are inputting in the string also has to be assigned the role you specified. </p><p>My final connection string looks like so:</p><code>"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&amp;role=validator"
</code><p>For Postgres, the connection string will be formatted like so:</p><code>postgresql://[username]:[password]@localhost:5432/[database]
</code><p>I’m using the username and password that I typically use to connect to my Postgres database. Unfortunately, this is a personal username and password which is not a best practice. It’s always best to use a user that is created for the use of external tools like data-diff. This helps keep your databases secure and ensures you are always in tight control of access.</p><p>My final connection string looks like so:</p><code>postgresql://madison:[password]@localhost:5432/development
</code><p>Now that you’ve built your connection strings, you can use them in the data-diff command, which looks like this:</p><code>data-diff DB1_URI TABLE1_NAME DB2_URI TABLE2_NAME
</code><p>You will input Snowflake’s connection string as DB1_URI and Postgres’s as DB2_URI.



Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.html

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
This will allow you to easily compare the differences between the source table and the destination table. Your data will be replicated to the database “development” and the schema “customers”, with the same table names. If the naming convention were different, it may get confusing down the line. </p><p>When you choose the streams you wish to activate, be sure that you are selecting “full refresh | overwrite”. This will capture any deletes or updates in your source data table through the replication process. </p><p>Select “Normalized tabular data” and create your connection. Once you set up your connection, you should see your data syncing. </p><p>‍</p><p>When your connection is finished syncing and reads “successful”, you are ready to begin the validation process! </p><h2>Validate the replication process with data-diff</h2><p>Now that you have your Postgres database replicated to Snowflake, you want to validate that your data looks as expected. Normally you would have to go through a long process of performing different discovery queries on each data source and comparing values. And, unless you looked at every single row and did a row-by-row comparison, you wouldn’t be confident that your data replicated as expected. This is where data-diff comes in. Let’s walk through how to set this up.</p><p>First, you’ll want to install the open-source tool. You can do this with the following command:</p><code>pip install data-diff
</code><p>Since you are using both Postgres and Snowflake, you want to install the drivers for these specific databases:</p><code>pip install 'data-diff[postgresql]'
pip install 'data-diff[snowflake]'
</code><p>After you’ve installed data-diff, you’ll have to focus on creating your connection strings. These contain key information about the database you are connecting to like the database user, password, host, account, database, schema, warehouse, and role.



Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.html

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
This will bring you to a screen where you can select your data source. Choose “<a>Postgres</a>” as your source type.</p><p>Now you will be brought to a screen where you need to enter some specific information about your Postgres database. This includes host, port, database name, and a list of the schemas you wish to sync. </p><p>I kept the default port and added my database named `development`, `customers` schema, and the login information for my Airbyte user. It is best practice to <a>create users specific to the tools</a>you are connecting to your database.</p><h3>Set up your Snowflake destination</h3><p>Now let’s set up our <a>Snowflake destination</a>where we will be replicating our Postgres data to. Start by clicking on “new destination” in the top right corner. Then select “Snowflake” as your destination type.</p><p>‍</p><p>This is where you will input the information for the Snowflake database that you are copying your Postgres data. Make sure you enter the right location information! </p><p>I also recommend setting up a role that is specific for loading data in your destination as well. This will help keep your environment secure and all you to closely monitor different metrics on the replication process.</p><h3>Set up a Postgres to Snowflake connection</h3><p>Now that you’ve created both your source in Postgres and your destination in Snowflake, you can set up a connection between the two to replicate your data from Postgres. Select “connections” on the left panel.</p><p>Select your Postgres source you created from the dropdown, then select Snowflake as your destination.</p><p>Now you’ll want to give the connection a good name and choose how often it replicates. I’m going to call mine “postgres_snowflake_replication” and set it t replicate every 24 hours. </p><p>I also recommend selecting “mirror source structure” for the “destination namespace”. This will allow you to easily compare the differences between the source table and the destination table.



Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.html

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
We can add this to our command like so:</p><code>data-diff \
"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&amp;role=validator" customer_contacts \
"postgresql://madison:[password]@localhost:5432/development" customer_contacts \
-columns customer_contact_id customer_name phone_number email_address updated_at \
-key-columns customer_contact_id \
-where “updated_at &lt;= '10-31-2022'"
</code><p>Now we are ready to run data-diff!</p><h3>Understanding data-diff output</h3><p>Once you run the command, data-diff will give you the primary key values of the rows in the tables that differ. Each value will have a + or - in front of it. A + signifies that the rows exists in the second table but not the first. A - sign indicates that the row exists in the first table but not the second. Now, you can dig deeper into your validation process and explore <em>why </em>these primary keys are in one database and not the other.</p><p>Also, note that one primary key can be present with a + and a - sign in the output. This means that the row <em>exists </em>in both tables but has a column value that varies. So not only does data-diff let you know when one table is missing a primary key, but it also lets you know when the column values for the rows with that primary key differ. </p><p>I can see here that the row with a `customer_contact_id` of 14339589 is present in Postgres but not Snowflake. I’ll want to look into why this row wasn’t replicated from my source table to my destination. This will involve a deeper dive into your data and understanding its behavior.



