Documentation Source:
docs.astronomer.io/astro/cli/test-your-astro-project-locally.txt

Documentation Title:
Test your Astro project | Astronomer Documentation

Documentation Content:
The report shows all Airflow providers and packages that have been removed, added, or updated.
When you read the results of this test, pay close attention to the
Major Updates
section. Major updates to Python packages are more likely to cause your DAGs to fail. Visit the changelog for any providers listed in this section (for example, the
HTTP provider changelog
) to see if the major upgrade will affect your environment. You should also pay attention to anything listed under
Unknown Updates
. These are updates that Astro CLI could not categorize, which can include major upgrades that might cause DAGs to break.
To run only the DAG test against the latest version of Astro Runtime, run the following command in your Astro Project:
astro dev upgrade-test --version-test
DAG test
​
When you upgrade, any Python packages that changed can generate import errors and cause your DAGs to break. These import errors are visible in the UI after you upgrade, but you can address them before upgrading by running the DAG test.
This test uses the
astro dev parse
command against the upgrade version and produces a report called
dag-test-report.html
in
upgrade-test-<current-version>--<upgrade-version>
. This HTML report lists the DAGs that will have import errors, along with the first error encountered if you complete an upgrade. You can use this report along with the dependency test report to fix errors in your DAGs before your upgrade.
To run only the DAG test against the latest version of Astro Runtime, run the following command in your Astro Project:
astro dev upgrade-test --dag-test
See also
​
Debug DAGs
astro dev pytest
Was this page helpful?
Yes
No
Sign up for Developer Updates
Get a summary of new Astro features once a month.
Submit
You can unsubscribe at any time.
By proceeding you agree to our
Privacy Policy
, our
Website Terms
and to receive emails from Astronomer.



Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
import_errors
.
items
(
)
]
@pytest
.
mark
.
parametrize
(
"rel_path,rv"
,
get_import_errors
(
)
,
ids
=
[
x
[
0
]
for
x
in
get_import_errors
(
)
]
)
def
test_file_imports
(
rel_path
,
rv
)
:
"""Test for import errors on a file"""
if
rel_path
and
rv
:
raise
Exception
(
f"
{
rel_path
}
failed to import with message \n
{
rv
}
"
)
Check for custom code requirements
​
Airflow DAGs support many types of custom plugins and code. It is common for data engineering teams to define best practices and custom rules around how their DAGs should be written and create DAG validation tests to ensure those standards are met.
The code snippet below includes a test which checks that all DAGs have their
tags
parameter set to one or more of the
APPROVED_TAGS
.
import
os
import
pytest
from
airflow
.
models
import
DagBag
def
get_dags
(
)
:
"""
Generate a tuple of dag_id, <DAG objects> in the DagBag
"""
dag_bag
=
DagBag
(
include_examples
=
False
)
def
strip_path_prefix
(
path
)
:
return
os
.
path
.
relpath
(
path
,
os
.
environ
.
get
(
"AIRFLOW_HOME"
)
)
return
[
(
k
,
v
,
strip_path_prefix
(
v
.
fileloc
)
)
for
k
,
v
in
dag_bag
.
dags
.
items
(
)
]
APPROVED_TAGS
=
{
"customer_success"
,
"op_analytics"
,
"product"
}
@pytest
.
mark
.
parametrize
(
"dag_id,dag,fileloc"
,
get_dags
(
)
,
ids
=
[
x
[
2
]
for
x
in
get_dags
(
)
]
)
def
test_dag_tags
(
dag_id
,
dag
,
fileloc
)
:
"""
test if a DAG is tagged and if those TAGs are in the approved list
"""
assert
dag
.



Documentation Source:
docs.astronomer.io/learn/debugging-dags.txt

Documentation Title:
Debug DAGs | Astronomer Documentation

Documentation Content:
Is called when defined with the
@dag
decorator. See also
Introduction to Airflow decorators
.
Import errors due to dependency conflicts
​
A frequent cause of DAG import errors is not having the necessary packages installed in your Airflow environment. You might be missing
provider packages
that are required for using specific operators or hooks, or you might be missing Python packages used in Airflow tasks.
In an Astro project, you can install OS-level packages by adding them to your
packages.txt
file. You can install Python-level packages, such as provider packages, by adding them to your
requirements.txt
file. If you need to install packages using a specific package manager, consider doing so by adding a bash command to your Dockerfile.
To prevent compatibility issues when new packages are released, Astronomer recommends pinning a package version to your project. For example, adding
astronomer-providers[all]==1.14.0
to your
requirements.txt
file ensures that no future releases of
astronomer-providers
causes compatibility issues. If no version is pinned, Airflow will always use the latest available version.
If you are using the Astro CLI, packages are installed in the scheduler Docker container. You can confirm that a package is installed correctly by running:
astro dev
bash
--scheduler
"pip freeze | grep <package-name>"
If you have conflicting package versions or need to run multiple Python versions, you can run tasks in different environments using a few different operators:
KubernetesPodOperator
: Runs a task in a separate Kubernetes Pod.
ExternalPythonOperator
: Runs a task in a predefined virtual environment.
PythonVirtualEnvOperator
: Runs a task in a temporary virtual environment.
If many Airflow tasks share a set of alternate package and version requirements a common pattern is to run them in two or more separate Airflow deployments.
DAGs are not running correctly
​
If your DAGs are either not running or running differently than you intended, consider checking the following common causes:
DAGs need to be unpaused in order to run on their schedule. You can unpause a DAG by clicking the toggle on the left side of the Airflow UI or by using the
Airflow CLI
.



Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
At a minimum, you should run DAG validation tests to check for
import errors
. Additional tests can check things like custom logic, ensuring that
catchup
is set to False for every DAG in your Airflow instance, or making sure only
tags
from a defined list are used in the DAGs.
DAG validation tests apply to all DAGs in your Airflow environment, so you only need to create one test suite.
Common DAG validation tests
​
This section covers the most common types of DAG validation tests with full code examples.
Check for import errors
​
The most common DAG validation test is to check for import errors. Checking for import errors through a validation test is faster than starting your Airflow environment and checking for errors in the Airflow UI. In the following test,
get_import_errors
checks the
.import_errors
attribute of the current
DagBag
.
import
os
import
pytest
from
airflow
.
models
import
DagBag
def
get_import_errors
(
)
:
"""
Generate a tuple for import errors in the dag bag
"""
dag_bag
=
DagBag
(
include_examples
=
False
)
def
strip_path_prefix
(
path
)
:
return
os
.
path
.
relpath
(
path
,
os
.
environ
.
get
(
"AIRFLOW_HOME"
)
)
# prepend "(None,None)" to ensure that a test object is always created even if it's a no op.
return
[
(
None
,
None
)
]
+
[
(
strip_path_prefix
(
k
)
,
v
.
strip
(
)
)
for
k
,
v
in
dag_bag
.
import_errors
.
items
(
)
]
@pytest
.
mark
.



