Documentation Source:
docs.astronomer.io/learn/airflow-datasets.txt

Documentation Title:
Datasets and data-aware scheduling in Airflow | Astronomer Documentation

Documentation Content:
readlines
(
)
cocktail
.
append
(
contents
)
return
[
item
for
sublist
in
cocktail
for
item
in
sublist
]
with
DAG
(
dag_id
=
"datasets_consumer_dag"
,
start_date
=
datetime
(
2022
,
10
,
1
)
,
schedule
=
[
INSTRUCTIONS
,
INFO
]
,
# Scheduled on both Datasets
catchup
=
False
,
)
:
PythonOperator
(
task_id
=
"read_about_cocktail"
,
python_callable
=
read_about_cocktail_func
,
)
Any number of datasets can be provided to the
schedule
parameter as a list or as an expression using
conditional logic
. If the Datasets are provided in a list, the DAG is triggered after all of the datasets have received at least one update due to a producing task completing successfully.
When you work with datasets, keep the following considerations in mind:
Datasets can only be used by DAGs in the same Airflow environment.
Airflow monitors datasets only within the context of DAGs and tasks. It does not monitor updates to datasets that occur outside of Airflow.
Consumer DAGs that are scheduled on a dataset are triggered every time a task that updates that dataset completes successfully. For example, if
task1
and
task2
both produce
dataset_a
, a consumer DAG of
dataset_a
runs twice - first when
task1
completes, and again when
task2
completes.
Consumer DAGs scheduled on a dataset are triggered as soon as the first task with that dataset as an outlet finishes, even if there are downstream producer tasks that also operate on the dataset.
Airflow 2.9 added several new features to datasets:
Conditional Dataset Scheduling
Combined Dataset and Time-based Scheduling
Datasets are now shown in the
Graph
view of a DAG in the Airflow UI. The
upstream1
DAG in the screenshot below is a consumer of the
dataset0
dataset, and has one task
update_dataset_1
that updates the
dataset1
dataset.
For more information about datasets, see
Data-aware scheduling
.



Documentation Source:
docs.astronomer.io/learn/airflow-datasets.txt

Documentation Title:
Datasets and data-aware scheduling in Airflow | Astronomer Documentation

Documentation Content:
write
(
msg
)
f
.
close
(
)
with
DAG
(
dag_id
=
"datasets_producer_dag"
,
start_date
=
datetime
(
2022
,
10
,
1
)
,
schedule
=
None
,
catchup
=
False
,
render_template_as_native_obj
=
True
,
)
:
get_cocktail
=
PythonOperator
(
task_id
=
"get_cocktail"
,
python_callable
=
get_cocktail_func
,
op_kwargs
=
{
"api"
:
API
}
,
)
write_instructions_to_file
=
PythonOperator
(
task_id
=
"write_instructions_to_file"
,
python_callable
=
write_instructions_to_file_func
,
op_kwargs
=
{
"response"
:
"{{ ti.xcom_pull(task_ids='get_cocktail') }}"
}
,
outlets
=
[
INSTRUCTIONS
]
,
)
write_info_to_file
=
PythonOperator
(
task_id
=
"write_info_to_file"
,
python_callable
=
write_info_to_file_func
,
op_kwargs
=
{
"response"
:
"{{ ti.xcom_pull(task_ids='get_cocktail') }}"
}
,
outlets
=
[
INFO
]
,
)
get_cocktail
>>
write_instructions_to_file
>>
write_info_to_file
A consumer DAG runs whenever the dataset(s) it is scheduled on is updated by a producer task, rather than running on a time-based schedule. For example, if you have a DAG that should run when the
INSTRUCTIONS
and
INFO
datasets are updated, you define the DAG's schedule using the names of those two datasets.
Any DAG that is scheduled with a dataset is considered a consumer DAG even if that DAG doesn't actually access the referenced dataset. In other words, it's up to you as the DAG author to correctly reference and use datasets.
TaskFlow API
Traditional syntax
from
pendulum
import
datetime
from
airflow
.
datasets
import
Dataset
from
airflow
.



Documentation Source:
docs.astronomer.io/learn/airflow-datasets.txt

Documentation Title:
Datasets and data-aware scheduling in Airflow | Astronomer Documentation

Documentation Content:
datasets
import
Dataset
from
airflow
.
decorators
import
dag
,
task
INSTRUCTIONS
=
Dataset
(
"file://localhost/airflow/include/cocktail_instructions.txt"
)
INFO
=
Dataset
(
"file://localhost/airflow/include/cocktail_info.txt"
)
@dag
(
dag_id
=
"datasets_consumer_dag"
,
start_date
=
datetime
(
2022
,
10
,
1
)
,
schedule
=
[
INSTRUCTIONS
,
INFO
]
,
# Scheduled on both Datasets
catchup
=
False
,
)
def
datasets_consumer_dag
(
)
:
@task
def
read_about_cocktail
(
)
:
cocktail
=
[
]
for
filename
in
(
"info"
,
"instructions"
)
:
with
open
(
f"include/cocktail_
{
filename
}
.txt"
,
"r"
)
as
f
:
contents
=
f
.
readlines
(
)
cocktail
.
append
(
contents
)
return
[
item
for
sublist
in
cocktail
for
item
in
sublist
]
read_about_cocktail
(
)
datasets_consumer_dag
(
)
from
pendulum
import
datetime
from
airflow
import
DAG
,
Dataset
from
airflow
.
operators
.
python
import
PythonOperator
INSTRUCTIONS
=
Dataset
(
"file://localhost/airflow/include/cocktail_instructions.txt"
)
INFO
=
Dataset
(
"file://localhost/airflow/include/cocktail_info.txt"
)
def
read_about_cocktail_func
(
)
:
cocktail
=
[
]
for
filename
in
(
"info"
,
"instructions"
)
:
with
open
(
f"include/cocktail_
{
filename
}
.txt"
,
"r"
)
as
f
:
contents
=
f
.
readlines
(
)
cocktail
.



Documentation Source:
docs.astronomer.io/learn/airflow-datasets.txt

Documentation Title:
Datasets and data-aware scheduling in Airflow | Astronomer Documentation

Documentation Content:
For more information about datasets, see
Data-aware scheduling
.
The
Datasets
tab, and the
DAG Dependencies
view in the Airflow UI give you observability for datasets and data dependencies in the DAG's schedule.
On the
DAGs
view, you can see that your
dataset_downstream_1_2
DAG is scheduled on two producer datasets (one in
dataset_upstream1
and
dataset_upstream2
). When Datasets are provided as a list, the DAG is scheduled to run after all Datasets in the list have received at least one update. In the following screenshot, the
dataset_downstream_1_2
DAG's next run is pending one dataset update. At this point the
dataset_upstream
DAG has run and updated its dataset, but the
dataset_upstream2
DAG has not.
The
Datasets
tab shows a list of all datasets in your Airflow environment and a graph showing how your DAGs and datasets are connected. You can filter the lists of Datasets by recent updates.
Click one of the datasets to display a list of task instances that updated the dataset and a highlighted view of that dataset and its connections on the graph.
The
DAG Dependencies
view (found under the
Browse
tab) shows a graph of all dependencies between DAGs (in green) and datasets (in orange) in your Airflow environment.
note
DAGs that are triggered by datasets do not have the concept of a data interval. If you need information about the triggering event in your downstream DAG, you can use the parameter
triggering_dataset_events
from the context. This parameter provides a list of all the triggering dataset events with parameters
[timestamp, source_dag_id, source_task_id, source_run_id, source_map_index ]
.
Updating a dataset
â€‹
As of Airflow 2.9+ there are three ways to update a dataset:
A task with an outlet parameter that references the dataset completes successfully.
A
POST
request to the
datasets endpoint of the Airflow REST API
.
A manual update in the Airflow UI.



