Documentation Source:
docs.astronomer.io/learn/debugging-dags.txt

Documentation Title:
Debug DAGs | Astronomer Documentation

Documentation Content:
If you want all DAGs unpaused by default, you can set
dags_are_paused_at_creation=False
in your Airflow config. If you do this, remember to set
catchup=False
in your DAGs to prevent automatic backfilling of DAG runs. Paused DAGs are unpaused automatically when you manually trigger them.
Double check that each DAG has a unique
dag_id
. If two DAGs with the same id are present in one Airflow instance the scheduler will pick one at random every 30 seconds to display.
Make sure your DAG has a
start_date
in the past. A DAG with a
start_date
in the future will result in a successful DAG run with no task runs. Do not use
datetime.now()
as a
start_date
.
Test the DAG using
astro dev dags test <dag_id>
. With the Airflow CLI, run
airflow dags test <dag_id>
.
If no DAGs are running, check the state of your scheduler
using
astro dev logs -s
.
If too many runs of your DAG are being scheduled after you unpause it, you most likely need to set
catchup=False
in your DAG's parameters.
If your DAG is running, but not on the schedule you expected, review the
DAG Schedule DAGs in Airflow
guide. If you are using a custom timetable, ensure that the data interval for your DAG run does not precede the DAG start date.
Common task issues
​
This section covers common issues related to individual tasks you might encounter. If your entire DAG is not working, see the
DAGs are not running correctly
section above.
Tasks are not running correctly
​
It is possible for a DAG to start but its tasks to be stuck in various states or to not run in the desired order. If your tasks are not running as intended, try the following debugging methods:
Double check that your DAG's
start_date
is in the past. A future
start_date
will result in a successful DAG run even though no tasks ran.
If your tasks stay in a
scheduled
or
queued
state, ensure your scheduler is running properly. If needed, restart the scheduler or increase scheduler resources in your Airflow infrastructure.



Documentation Source:
docs.astronomer.io/learn/rerunning-dags.txt

Documentation Title:
Rerun Airflow DAGs and tasks | Astronomer Documentation

Documentation Content:
If you have a small number of DAG runs to backfill, you can trigger them manually from the Airflow UI via
Trigger DAG w/ config
and choose the desired logical date as shown in the following image:
Was this page helpful?
Yes
No
Sign up for Developer Updates
Get a summary of new Astro features once a month.
Submit
You can unsubscribe at any time.
By proceeding you agree to our
Privacy Policy
, our
Website Terms
and to receive emails from Astronomer.
Edit this page
Previous
Pass data between tasks
Next
SubDAGs
Assumed knowledge
Automatically retry tasks
Automatically pause a failing DAG
Manually rerun tasks or DAGs
Add notes to cleared tasks and DAGs
Clear all tasks
Catchup
Backfill
Legal
·
Privacy
·
Security
·
Cookie Preferences
© Astronomer 2023. Various trademarks held by their respective owners.



Documentation Source:
docs.astronomer.io/learn/testing-airflow.txt

Documentation Title:
Test Airflow DAGs | Astronomer Documentation

Documentation Content:
PyCharm
.
Tools like
The Python Debugger
and the built-in
breakpoint()
function. These allow you to run
dag.test()
from the command line by running
python <path-to-dag-file>
.
Use
dag.test()
with the Astro CLI
​
If you use the Astro CLI exclusively and do not have the
airflow
package installed locally, you can still debug using
dag.test()
by running
astro dev start
, entering the scheduler container with
astro dev bash -s
, and executing
python <path-to-dag-file>
from within the Docker container. Unlike using the base
airflow
package, this testing method requires starting up a complete Airflow environment.
Use variables and connections in dag.test()
​
To debug your DAGs in a more realistic environment, you can pass the following Airflow environment configurations to
dag.test()
:
execution_date
passed as a
pendulum.datetime
object.
Airflow connections
passed as a
.yaml
file.
Airflow variables passed as a
.yaml
file.
DAG configuration passed as a dictionary.
This is useful for testing your DAG for different dates or with different connections and configurations. The following code snippet shows the syntax for passing various parameters to
dag.test()
:
from
pendulum
import
datetime
if
__name__
==
"__main__"
:
conn_path
=
"connections.yaml"
variables_path
=
"variables.yaml"
my_conf_var
=
23
dag
.



Documentation Source:
docs.astronomer.io/astro/cli/troubleshoot-locally.txt

Documentation Title:
Troubleshoot a local Airflow environment | Astronomer Documentation

Documentation Content:
Run the following command to build your Astro project into a Docker image and start a local Docker container for each Airflow component:
astro dev start
Run the following command to open a bash terminal in your scheduler container:
astro dev
bash
--scheduler
In the bash terminal for your container, run the following command to install a package and review any error messages that are returned:
apt-get
install
<
package-name
>
For example, to install the GNU Compiler Collection (GCC) compiler, you would run:
apt-get
install
gcc
Open your Astro project
packages.txt
file and add the package references you removed in Step 1 individually until you find the package that is the source of the error.
New DAGs aren't visible in the Airflow UI
​
Make sure that no DAGs have duplicate
dag_ids
. When two DAGs use the same
dag_id
, the newest DAG won't appear in the Airflow UI and you won't receive an error message.
By default, the Airflow scheduler scans the
dags
directory of your Astro project for new files every 300 seconds (5 minutes). For this reason, it might take a few minutes for new DAGs to appear in the Airflow UI. Changes to existing DAGs appear immediately.
To have the scheduler check for new DAGs more frequently, you can set the
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL
environment variable to less than 300 seconds. If you have less than 200 DAGs in a Deployment, it's safe to set
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL
to
30
(30 seconds). See
Set environment variables
for how to set this on Astro.
In Astro Runtime 7.0 and later, the Airflow UI
Code
page includes a
Parsed at
value which shows when a DAG was last parsed. This value can help you determine when a DAG was last rendered in the Airflow UI. To view the
Parsed at
value in the Airflow UI, click
DAGs
, select a DAG, and then click
Code
. The
Parsed at
value appears at the top of the DAG code pane.



