Documentation Source:
docs.astronomer.io/astro/airflow-api.txt

Documentation Title:
Make requests to the Airflow REST API | Astronomer Documentation

Documentation Content:
If it's not, add
apache-airflow-providers-http
to the
requirements.txt
file of our Astro project and redeploy it to Astro.
In your triggering DAG, add the following task. It uses the
SimpleHttpOperator
to make a request to the
dagRuns
endpoint of the Deployment that contains the DAG to trigger.
from
datetime
import
datetime
from
airflow
.
models
.
dag
import
DAG
from
airflow
.
providers
.
http
.
operators
.
http
import
SimpleHttpOperator
with
DAG
(
dag_id
=
"triggering_dag"
,
schedule
=
None
,
start_date
=
datetime
(
2023
,
1
,
1
)
)
:
SimpleHttpOperator
(
task_id
=
"trigger_external_dag"
,
log_response
=
True
,
method
=
"POST"
,
# Change this to the DAG_ID of the DAG you are triggering
endpoint
=
f"api/v1/dags/<triggered_dag>/dagRuns"
,
http_conn_id
=
"http_conn"
,
data
=
{
"logical_date"
:
"{{ logical_date }}"
,
# if you want to add parameters:
# params: '{"foo": "bar"}'
}
)
Was this page helpful?
Yes
No
Sign up for Developer Updates
Get a summary of new Astro features once a month.
Submit
You can unsubscribe at any time.
By proceeding you agree to our
Privacy Policy
, our
Website Terms
and to receive emails from Astronomer.
Edit this page
Previous
Upgrade Astro Runtime
Next
Overview
Prerequisites
Step 1: Retrieve your access token
Step 2: Retrieve the Deployment URL
Step 3: Make an Airflow API request
Example API Requests
List DAGs
Trigger a DAG run
Trigger a DAG run by date
Pause a DAG
Trigger DAG runs across Deployments
Legal
·
Privacy
·
Security
·
Cookie Preferences
© Astronomer 2023. Various trademarks held by their respective owners.



Documentation Source:
docs.astronomer.io/learn/get-started-with-airflow-part-2.txt

Documentation Title:
Get started with Apache Airflow, Part 2: Providers, connections, and variables | Astronomer Documentation

Documentation Content:
Save the connection by clicking the
Save
button.
Note that the option to test connections is only available for selected connection types and disabled by default in Airflow 2.7+, see
Test a connection
.
Step 5: Create an HTTP connection
​
In the
Connections
view, click
+
to create a new connection.
Name the connection
open_notify_api_conn
and select a
Connection Type
of
HTTP
.
Enter the host URL for the API you want to query in the
Host
field. For this tutorial we use the
Open Notify API
, which has an endpoint returning the current location of the ISS. The host for this API is
http://api.open-notify.org
.
Click
Save
.
You should now have two connections as shown in the following screenshot:
Step 6: Review the DAG code
​
Now that your Airflow environment is configured correctly, look at the DAG code you copied from the repository to see how your new variable and connections are used at the code level.
At the top of the file, the DAG is described in a docstring. It's highly recommended to always document your DAGs and include any additional connections or variables that are required for the DAG to work.
"""
## Find the International Space Station
This DAG waits for a specific commit message to appear in a GitHub repository,
and then pulls the current location of the International Space Station from an API
and print it to the logs.
This DAG needs a GitHub connection with the name `my_github_conn` and
an HTTP connection with the name `open_notify_api_conn`
and the host `https://api.open-notify.org/` to work.
Additionally you need to set an Airflow variable with
the name `open_notify_api_endpoint` and the value `iss-now.json`.
"""
After the docstring, all necessary packages are imported. Notice how both the HttpOperator as well as the GithubSensor are part of provider packages.
from
airflow
.
decorators
import
dag
,
task
from
airflow
.
models
.
baseoperator
import
chain
from
airflow
.
providers
.
http
.
operators
.



Documentation Source:
docs.astronomer.io/astro/airflow-api.txt

Documentation Title:
Make requests to the Airflow REST API | Astronomer Documentation

Documentation Content:
Replace
<your-dag-id>
with your own value.
cURL
​
curl
-X
PATCH https://
<
your-deployment-url
>
/api/v1/dags/
<
your-dag-id
>
\
-H
'Content-Type: application/json'
\
-H
'Cache-Control: no-cache'
\
-H
'Authorization: Bearer <your-access-token>'
\
-d
'{"is_paused": true}'
Python
​
import
requests
token
=
"<your-access-token>"
deployment_url
=
"<your-deployment-url>"
dag_id
=
"<your-dag-id>"
response
=
requests
.
patch
(
url
=
f"https://
{
deployment_url
}
/api/v1/dags/
{
dag_id
}
"
,
headers
=
{
"Authorization"
:
f"Bearer
{
token
}
"
,
"Content-Type"
:
"application/json"
}
,
data
=
'{"is_paused": true}'
)
print
(
response
.
json
(
)
)
# Prints data about the DAG with id <dag-id>
Trigger DAG runs across Deployments
​
You can use the Airflow REST API to make a request in one Deployment that triggers a DAG run in a different Deployment. This is sometimes necessary when you have interdependent workflows across multiple Deployments. On Astro, you can do this for any Deployment in any Workspace or cluster.
This topic has guidelines on how to trigger a DAG run, but you can modify the example DAG provided to trigger any request that's supported in the Airflow REST API.
Create a
Deployment API token
for the Deployment that contains the DAG you want to trigger.
In the Deployment that contains the triggering DAG, create an
Airflow HTTP connection
with the following values:
Connection Id
:
http_conn
Connection Type
: HTTP
Host
:
<your-deployment-url>
Schema
:
https
Extras
:
{
"Content-Type"
:
"application/json"
,
"Authorization"
:
"Bearer <your-deployment-api-token>"
}
See
Manage connections in Apache Airflow
.
info
If the
HTTP
connection type is not available, double check that the
HTTP provider
is installed in your Airflow environment.



Documentation Source:
docs.astronomer.io/learn/airflow-fivetran.txt

Documentation Title:
Use Fivetran with Apache Airflow | Astronomer Documentation

Documentation Content:
Click
Generate API Key
. Copy the API key information to a secure place for later.
Step 6: Create an Airflow connection to Fivetran
​
In a web browser, go to
localhost:8080
to access the Airflow UI.
Click
Admin
->
Connections
->
+
to create a new connection.
Name your connection
fivetran_conn
and select the
Fivetran
connection type. Provide your Fivetran API key and Fivetran API secret. If the Fivetran connection type isn't available, try restarting your Airflow instance with
astro dev restart
to ensure the contents of
requirements.txt
have been installed.
Click
Save
.
Step 7: Create your Airflow DAG
​
For this tutorial you will create a DAG that triggers your Fivetran sync to ingest the GitHub repo metadata to your destination.
Open your
astro-fivetran-project
in a code-editor.
In your
dags
folder add a new Python file called
my_fivetran_dag.py
.
Copy and paste the following DAG code into the file:
from
airflow
.
decorators
import
dag
,
task
from
pendulum
import
datetime
from
fivetran_provider_async
.



