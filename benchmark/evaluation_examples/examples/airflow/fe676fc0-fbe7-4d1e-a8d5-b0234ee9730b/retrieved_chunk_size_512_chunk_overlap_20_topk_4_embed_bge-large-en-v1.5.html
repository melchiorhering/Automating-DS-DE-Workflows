Documentation Source:
docs.astronomer.io/learn/debugging-dags.html

Documentation Title:
Debug DAGs | Astronomer Documentation

Documentation Content:
You can enable connection testing by defining the environment variable <code>AIRFLOW__CORE__TEST_CONNECTION=Enabled</code>in your Airflow environment. Astronomer recommends not enabling this feature until you are sure that only highly trusted UI/API users have "edit connection" permissions.</p></div><p>To find information about what parameters are required for a specific connection:</p><ul><li>Read provider documentation in the <a>Astronomer Registry</a>to access the Apache Airflow documentation for the provider. Most commonly used providers will have documentation on each of their associated connection types. For example, you can find information on how to set up different connections to Azure in the Azure provider docs.</li><li>Check the documentation of the external tool you are connecting to and see if it offers guidance on how to authenticate.</li><li>View the source code of the hook that is being used by your operator.</li></ul><p>You can also test connections from within your IDE by using the <code>dag.test()</code>method. See <a>Debug interactively with dag.test()</a>and <a>How to test and debug your Airflow connections</a>.</p><h2>I need more help<a>​</a></h2><p>The information provided here should help you resolve the most common issues. If your issue was not covered in this guide, try the following resources:</p><ul><li>If you are an Astronomer customer contact our <a>customer support</a>.</li><li>Post your question to <a>Stack Overflow</a>, tagged with <code>airflow</code>and other relevant tools you are using. Using Stack Overflow is ideal when you are unsure which tool is causing the error, since experts for different tools will be able to see your question.</li><li>Join the <a>Apache Airflow Slack</a>and open a thread in <code>#newbie-questions</code>or <code>#troubleshooting</code>. The Airflow slack is the best place to get answers to more complex Airflow specific questions.</li><li>If you found a bug in Airflow or one of its core providers, please open an issue in the <a>Airflow GitHub repository</a>.



Documentation Source:
docs.astronomer.io/learn/airflow-sql-data-quality.html

Documentation Title:
Run data quality checks using SQL check operators | Astronomer Documentation

Documentation Content:
Note that currently the operators cannot support BigQuery <code>job_id</code>s.</li><li>A love for birds.</li></ul><h2>Step 1: Configure your Astro project<a>​</a></h2><p>To use SQL check operators, install the <a>Common SQL provider</a>in your Astro project.</p><ol><li><p>Run the following commands to create a new Astro project:</p><code><span><span>$ </span><span>mkdir</span><span>astro-sql-check-tutorial </span><span>&amp;&amp;</span><span>cd</span><span>astro-sql-check-tutorial</span></span><span>$ astro dev init</span></code></li><li><p>Add the Common SQL provider and the SQLite provider to your Astro project <code>requirements.txt</code>file.</p><code><span>apache-airflow-providers-common-sql==1.5.2</span><span>apache-airflow-providers-sqlite==3.4.2</span></code></li></ol><h2>Step 2: Create a connection to SQLite<a>​</a></h2><ol><p>In the Airflow UI, go to <strong>Admin</strong>&gt; <strong>Connections</strong>and click <strong>+</strong>.</p><li><p>Create a new connection named <code>sqlite_conn</code>and choose the <code>SQLite</code>connection type. Enter the following information:</p><ul><li><strong>Connection Id</strong>: <code>sqlite_conn</code>.</li><li><strong>Connection Type</strong>: <code>SQLite</code>.</li><li><strong>Host</strong>: <code>/tmp/sqlite.db</code>.</li></ul></li></ol><h2>Step 3: Add a SQL file with a custom check<a>​</a></h2><ol><p>In your <code>include</code>folder, create a file called <code>custom_check.sql</code>.



Documentation Source:
docs.astronomer.io/learn/debugging-dags.html

Documentation Title:
Debug DAGs | Astronomer Documentation

Documentation Content:
This is often related to either lack of resources or an error in the task configuration.</li><li>Increase the CPU or memory for the task.</li><li>Ensure that your logs are retained until you need to access them. If you are an Astronomer customer see our documentation on how to <a>View logs</a>.</li><li>Check your scheduler and webserver logs for any errors that might indicate why your task logs aren't appearing.</li></ul><h2>Troubleshooting connections<a>​</a></h2><p>Typically, Airflow connections are needed to allow Airflow to communicate with external systems. Most hooks and operators expect a defined connection parameter. Because of this, improperly defined connections are one of the most common issues Airflow users have to debug when first working with their DAGs.</p><p>While the specific error associated with a poorly defined connection can vary widely, you will typically see a message with "connection" in your task logs. If you haven't defined a connection, you'll see a message such as <code>'connection_abc' is not defined</code>.</p><p>The following are some debugging steps you can try:</p><ul><p>Review <a>Manage connections in Apache Airflow</a>to learn how connections work.</p><p>Make sure you have the necessary provider packages installed to be able to use a specific connection type.</p><p>Change the <code>&lt;external tool&gt;_default</code>connection to use your connection details or define a new connection with a different name and pass the new name to the hook or operator.</p><p>Define connections using Airflow environment variables instead of adding them in the Airflow UI. Make sure you're not defining the same connection in multiple places. If you do, the environment variable takes precedence.</p><p>Test if your credentials work when used in a direct API call to the external tool.</p><p>Test your connections using the Airflow UI or the Airflow CLI. See <a>Testing connections</a>.</p></ul><div><div>note</div><p>Testing connections is disabled by default in Airflow 2.7+.



Documentation Source:
docs.astronomer.io/learn/airflow-databricks.html

Documentation Title:
Orchestrate Databricks jobs with Airflow | Astronomer Documentation

Documentation Content:
If you would be using Airflow's built in <a>retry functionality</a>a separete cluster would be created for each failed task.</p><p>If you only want to rerun specific tasks within your Workflow, you can use the <strong>Repair a single failed task</strong>operator extra link on an individual task in the Databricks Workflow.</p><h2>Alternative ways to run Databricks with Airflow<a>​</a></h2><p>The Astro Databricks provider is under active development, and support for more Databricks task types is still being added. If you want to orchestrate an action in your Databricks environment that is not yet supported by the Astro Databricks provider such as <a>updating a Databricks repository</a>, check the <a>community-managed Databricks provider</a>for relevant operators.</p><p>Additionally, the community-managed Databricks provider contains hooks (for example the <a>DatabricksHook</a>) that simplify interaction with Databricks, including writing your own <a>custom Databricks operators</a>.</p><p>You can find several example DAGs that use the community-managed Databricks provider on the <a>Astronomer Registry</a>.</p></div><div><h2>Was this page helpful?</h2><div><button>Yes</button><button>No</button></div></div><form><h2>Sign up for Developer Updates</h2><p>Get a summary of new Astro features once a month.</p><button>Submit</button><p>You can unsubscribe at any time.



