Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv.html

Documentation Title:
Loading CSV data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
</p><p>To authenticate to BigQuery, set up Application Default Credentials.
      For more information, see
      
        <a>Set up authentication for client libraries</a>.
      
    </p><code>require "google/cloud/bigquery"

def load_table_gcs_csv dataset_id = "your_dataset_id"
  bigquery = Google::Cloud::Bigquery.new
  dataset  = bigquery.dataset dataset_id
  gcs_uri  = "gs://cloud-samples-data/bigquery/us-states/us-states.csv"
  table_id = "us_states"

  load_job = dataset.load_job table_id, gcs_uri, skip_leading: 1 do |schema|
    schema.string "name"
    schema.string "post_abbr"
  end
  puts "Starting job #{load_job.job_id}"

  load_job.wait_until_done! # Waits for table load to complete.
  puts "Job finished."

  table = dataset.table table_id
  puts "Loaded #{table.rows_count} rows to table #{table.id}"
end</code></section></section></div><h2>Loading CSV data into a table that uses column-based time partitioning</h2><p>To load CSV data from Cloud Storage into a BigQuery table
that uses column-based time partitioning:</p><div><section><span>Go</span><p>Before trying this sample, follow the <span>Go</span>setup instructions in the
          <a>BigQuery quickstart using
            client libraries</a>.
        
      
      
  For more information, see the
  <a>BigQuery <span>Go</span>API
    reference documentation</a>.
  
    </p><p>To authenticate to BigQuery, set up Application Default Credentials.
      For more information, see
      
        <a>Set up authentication for client libraries</a>.
      
    </p><code>import (
	"context"
	"fmt"
	"time"

	"cloud.google.com/go/bigquery"
)

// importPartitionedTable demonstrates specifing time partitioning for a BigQuery table when loading
// CSV data from Cloud Storage.



Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv.html

Documentation Title:
Loading CSV data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
</li><li>In the <b>Dataset info</b>section, click <span>add_box</span><b>Create table</b>.</li><li>In the <b>Create table</b>panel, specify the following details: </li><ol><li>In the <b>Source</b>section, select <b>Google Cloud Storage</b>in the <b>Create table from</b>list.
  Then, do the following:
  <ol><li>Select a file from the Cloud Storage bucket, or enter the
              <a>Cloud Storage URI</a>.
              You cannot include multiple URIs
              in the Google Cloud console, but <a>wildcards</a>are supported. The Cloud Storage bucket must be in the same
              location as the dataset that contains the table you want to create, append, or
              overwrite.
      

      

</li><li>For <b>File format</b>, select
       <b>CSV</b>.
      </li></ol></li><li>In the <b>Destination</b>section, specify the following details:
<ol><li>For <b>Dataset</b>, select the dataset in which you want to create the
        table.</li><li>In the <b>Table</b>field, enter the name of the table that you want to create.</li><li>Verify that the <b>Table type</b>field is set to <b>Native table</b>.</li></ol></li><li>In the <b>Schema</b>section, enter the <a>schema</a>definition.
   To enable the <a>auto detection</a>of a schema,
  select <b>Auto detect</b>.



  

  You can enter schema information manually by using one of
     the following methods:
<ul><li>Option 1: Click <b>Edit as text</b>and paste the schema in the form of a
        JSON array. When you use a JSON array, you generate the schema using the
        same process as <a>creating a JSON schema file</a>.



Documentation Source:
cloud.google.com/bigquery/docs/samples/bigquerydatatransfer-create-ads-transfer.html

Documentation Title:
Load data from Google Ads  |  BigQuery  |  Google Cloud

Documentation Content:
table properties</span><span>Get view properties</span><span>Grant view access</span><span>Import a local file</span><span>Insert GeoJSON data</span><span>Insert rows with no IDs</span><span>Insert WKT data</span><span>List by label</span><span>List datasets</span><span>List jobs</span><span>List models</span><span>List models using streaming</span><span>List routines</span><span>List tables</span><span>Load a CSV file</span><span>Load a CSV file to replace a table</span><span>Load a CSV file with autodetect schema</span><span>Load a DataFrame to BigQuery with pandas-gbq</span><span>Load a JSON file</span><span>Load a JSON file to replace a table</span><span>Load a JSON file with autodetect schema</span><span>Load a Parquet file</span><span>Load a Parquet to replace a table</span><span>Load a table in JSON format</span><span>Load an Avro file</span><span>Load an Avro file to replace a table</span><span>Load an ORC file</span><span>Load an ORC file to replace a table</span><span>Load data from DataFrame</span><span>Load data into a column-based time partitioning table</span><span>Migration Guide: pandas-gbq</span><span>Migration Guide: pandas-gbq</span><span>Named parameters</span><span>Named parameters and provided types</span><span>Nested repeated schema</span><span>Positional parameters</span><span>Positional parameters and provided types</span><span>Preview table data</span><span>Query a clustered table</span><span>Query a column-based time-partitioned table</span><span>Query a table</span><span>Query Bigtable using a permanent table</span><span>Query Bigtable using a temporary table</span><span>Query Cloud Storage with a permanent table</span><span>Query Cloud Storage with a temporary table</span><span>Query materialized view</span><span>Query



Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv.html

Documentation Title:
Loading CSV data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
</p><code>use Google\Cloud\BigQuery\BigQueryClient;
use Google\Cloud\Core\ExponentialBackoff;

/** Uncomment and populate these variables in your code */
// $projectId  = 'The Google project ID';
// $datasetId  = 'The BigQuery dataset ID';

// instantiate the bigquery table service
$bigQuery = new BigQueryClient([
    'projectId' =&gt; $projectId,
]);
$dataset = $bigQuery-&gt;dataset($datasetId);
$table = $dataset-&gt;table('us_states');

// create the import job
$gcsUri = 'gs://cloud-samples-data/bigquery/us-states/us-states.csv';
$schema = [
    'fields' =&gt; [
        ['name' =&gt; 'name', 'type' =&gt; 'string'],
        ['name' =&gt; 'post_abbr', 'type' =&gt; 'string']
    ]
];
$loadConfig = $table-&gt;loadFromStorage($gcsUri)-&gt;schema($schema)-&gt;skipLeadingRows(1);
$job = $table-&gt;runJob($loadConfig);
// poll the job until it is complete
$backoff = new ExponentialBackoff(10);
$backoff-&gt;execute(function () use ($job) {
    print('Waiting for job to complete' . PHP_EOL);
    $job-&gt;reload();
    if (!$job-&gt;isComplete()) {
        throw new Exception('Job has not yet completed', 500);
    }
});
// check if the job has errors
if (isset($job-&gt;info()['status']['errorResult'])) {
    $error = $job-&gt;info()['status']['errorResult']['message'];
    printf('Error running job: %s' . PHP_EOL, $error);
} else {
    print('Data imported successfully' . PHP_EOL);
}</code></section></section><section><h3>Python </h3><section><p>Before trying this sample, follow the <span>Python</span>setup instructions in the
          <a>BigQuery quickstart using
            client libraries</a>.



