Documentation Source:
cloud.google.com/bigquery/docs/google-ads-transfer.txt

Documentation Title:
Google Ads transfers  |  BigQuery  |  Google Cloud

Documentation Content:
Create Google Ads data transfer
To create a data transfer for Google Ads reporting, you need
either your Google Ads customer ID or your manager account (MCC).
For information about retrieving your Google Ads customer ID, see
Find your Customer ID
.
To create a data transfer for Google Ads reporting, select one of the following options:
Console
Go to the BigQuery page in the Google Cloud console.
Go to the BigQuery page
Click
sync_alt
Data transfers
.
Click
add
Create transfer
.
In the
Source type
section, for
Source
, choose
Google Ads
.
In the
Transfer config name
section, for
Display name
, enter a
name for the transfer such as
My Transfer
. The transfer name can be
any value that lets you identify the transfer if you need
to modify it later.
In the
Schedule options
section:
For
Repeat frequency
, choose an option for how often to run the
transfer. If you select
Days
, provide a valid time in UTC.
Hours
Days
On-demand
If applicable, select either
Start now
or
Start at set time
and provide a start date and run time.
In the
Destination settings
section, for
Dataset
,
select the dataset that you created to store your data.
In the
Data source details
section:
For
Customer ID
, enter your Google Ads customer ID:
Optional: Select options to exclude removed or deactivated items and
include tables new to Google Ads.
Optional: Enter a comma-separated list of tables to include, for example
Campaign, AdGroup
. Prefix this list with the
-
character to exclude certain
tables, for example
-Campaign, AdGroup
. All tables are included by
default.
Optional: Select the option to include tables specific to PMax
reports. For more information about PMax support, see
PMax
support
.
Optional: For
Refresh window
, enter a value between 1 and 30.
In the
Service Account
menu, select a
service account
from the service accounts associated with your
Google Cloud project. You can associate a service account with
your transfer instead of using your user credentials.



Documentation Source:
cloud.google.com/bigquery/docs/samples/bigquerydatatransfer-create-ads-transfer.txt

Documentation Title:
Load data from Google Ads  |  BigQuery  |  Google Cloud

Documentation Content:
Load data from Google Ads  |  BigQuery  |  Google Cloud
Documentation
Technology areas
close
AI solutions, generative AI, and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
close
Access and resources management
Cloud SDK, languages, frameworks, and tools
Costs and usage management
Infrastructure as code
Migration
Related sites
close
Google Cloud Home
Free Trial and Free Tier
Architecture Center
Blog
Contact Sales
Google Cloud Developer Center
Google Developer Center
Google Cloud Marketplace (in console)
Google Cloud Marketplace Documentation
Google Cloud Skills Boost
Google Cloud Solution Center
Google Cloud Support
Google Cloud Tech Youtube Channel
English
Deutsch
Español – América Latina
Français
Indonesia
Italiano
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
BigQuery
Guides
Reference
Samples
Resources
Contact Us
Start free
Documentation
Guides
Reference
Samples
Resources
Technology areas
More
Cross-product tools
More
Related sites
More
Console
Contact Us
Start free
BigQuery
All BigQuery code samples
Analytics Hub Samples
Create a data exchange and listing using Analytics Hub
BigQuery Samples
Create a BigQuery DataFrame from a CSV file in GCS
Create a BigQuery DataFrame from a finished query job
Add a column using a load job
Add a column using a query job
Add a label
Add an empty column
Array parameters
Authorize a BigQuery Dataset
Cancel a job
Check dataset existence
Clustered table
Column-based time partitioning
Copy a single-source table
Copy a table
Copy multiple tables
Create a BigQuery DataFrame from a table
Create a client with a service account key file
Create a client with application default credentials
Create a clustered table
Create a clustering model with BigQuery DataFrames
Create a dataset and grant access to it
Create a dataset in BigQuery.
Create a dataset with a customer-managed encryption key
Create a job
Create a model
Create a



Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv.txt

Documentation Title:
Loading CSV data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
For more information, see
Set up authentication for client libraries
.
use Google\Cloud\BigQuery\BigQueryClient;
use Google\Cloud\Core\ExponentialBackoff;

/** Uncomment and populate these variables in your code */
// $projectId  = 'The Google project ID';
// $datasetId  = 'The BigQuery dataset ID';

// instantiate the bigquery table service
$bigQuery = new BigQueryClient([
    'projectId' => $projectId,
]);
$dataset = $bigQuery->dataset($datasetId);
$table = $dataset->table('us_states');

// create the import job
$gcsUri = 'gs://cloud-samples-data/bigquery/us-states/us-states.csv';
$schema = [
    'fields' => [
        ['name' => 'name', 'type' => 'string'],
        ['name' => 'post_abbr', 'type' => 'string']
    ]
];
$loadConfig = $table->loadFromStorage($gcsUri)->schema($schema)->skipLeadingRows(1);
$job = $table->runJob($loadConfig);
// poll the job until it is complete
$backoff = new ExponentialBackoff(10);
$backoff->execute(function () use ($job) {
    print('Waiting for job to complete' . PHP_EOL);
    $job->reload();
    if (!$job->isComplete()) {
        throw new Exception('Job has not yet completed', 500);
    }
});
// check if the job has errors
if (isset($job->info()['status']['errorResult'])) {
    $error = $job->info()['status']['errorResult']['message'];
    printf('Error running job: %s' . PHP_EOL, $error);
} else {
    print('Data imported successfully' . PHP_EOL);
}
Python
Before trying this sample, follow the
Python
setup instructions in the
BigQuery quickstart using
            client libraries
.
        
      
      
  For more information, see the
BigQuery
Python
API
    reference documentation
.
To authenticate to BigQuery, set up Application Default Credentials.
      For more information, see
Set up authentication for client libraries
.



Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv.txt

Documentation Title:
Loading CSV data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
Loading CSV data into a table
To load CSV data from Cloud Storage into a new BigQuery
table, select one of the following options:
Console
To follow step-by-step guidance for this task directly in the
  Cloud Shell Editor, click
Guide me
:
Guide me
In the Google Cloud console, go to the
BigQuery
page.
Go to BigQuery
In the
Explorer
pane, expand your project, and then select a dataset.
In the
Dataset info
section, click
add_box
Create table
.
In the
Create table
panel, specify the following details:
In the
Source
section, select
Google Cloud Storage
in the
Create table from
list.
  Then, do the following:
Select a file from the Cloud Storage bucket, or enter the
Cloud Storage URI
.
              You cannot include multiple URIs
              in the Google Cloud console, but
wildcards
are supported. The Cloud Storage bucket must be in the same
              location as the dataset that contains the table you want to create, append, or
              overwrite.
For
File format
, select
CSV
.
In the
Destination
section, specify the following details:
For
Dataset
, select the dataset in which you want to create the
        table.
In the
Table
field, enter the name of the table that you want to create.
Verify that the
Table type
field is set to
Native table
.
In the
Schema
section, enter the
schema
definition.
   To enable the
auto detection
of a schema,
  select
Auto detect
.



  

  You can enter schema information manually by using one of
     the following methods:
Option 1: Click
Edit as text
and paste the schema in the form of a
        JSON array. When you use a JSON array, you generate the schema using the
        same process as
creating a JSON schema file
.
        You can view the schema of an existing table in JSON format by entering the following
      command:
bq show
--format=prettyjson
dataset.table
Option 2: Click
add_box
Add field
and enter the table schema.



