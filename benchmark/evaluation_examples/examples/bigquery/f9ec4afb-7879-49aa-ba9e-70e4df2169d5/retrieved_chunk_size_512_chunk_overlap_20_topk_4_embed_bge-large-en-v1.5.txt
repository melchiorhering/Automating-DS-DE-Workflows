Documentation Source:
cloud.google.com/bigquery/docs/inference-tutorial-resnet.txt

Documentation Title:
Tutorial: Run inference on an object table by using a classification model  |  BigQuery  |  Google Cloud

Documentation Content:
data quality
Monitor Data Transfer Service
Monitor materialized views
Monitor reservations
Dashboards, charts and alerts
Audit workloads
Introduction
Audit policy tags
View Data Policy audit logs
Data Transfer Service audit logs
Analytics Hub audit logging
BigQuery audit logs reference
Migrate audit logs
BigLake API audit logs
Optimize resources
Control costs
Estimate and control query costs
Custom cost controls
Optimize with recommendations
View cluster and partition recommendations
Apply cluster and partition recommendations
Manage materialized view recommendations
Organize with labels
Introduction
Add labels
View labels
Update labels
Filter using labels
Delete labels
Manage data quality
Monitor data quality with scans
Data Catalog overview
Work with Data Catalog
Govern
Introduction
Control access to resources
Introduction
Control access to resources with IAM
Control access with authorization
Authorized datasets
Authorized routines
Authorized views
Control access with VPC service controls
Control table and dataset access with tags
Control access with conditions
Control column and row access
Control access to table columns
Introduction to column-level access control
Restrict access with column-level access control
Impact on writes
Manage policy tags
Manage policy tags across locations
Best practices for using policy tags
Control access to table rows
Introduction to row-level security
Work with row-level security
Use row-level security with other BigQuery features
Best practices for row-level security
Protect sensitive data
Mask data in table columns
Introduction to data masking
Mask column data
Anonymize data with differential privacy
Use differential privacy
Extend differential privacy
Restrict data access using analysis rules
Use Sensitive Data Protection
Manage encryption
Encryption at rest
Customer-managed encryption keys
Column-level encryption with Cloud KMS
AEAD encryption
Develop
Introduction
BigQuery code samples
BigQuery API basics
BigQuery APIs and libraries overview
Authentication
Introduction
Get started
Authenticate as an end user
Authenticate with JSON Web Tokens
Run jobs programmatically
Paginate with BigQuery API
API performance tips
Batch requests
Choose a Python library
BigQuery DataFrames
Introduction
Use BigQuery DataFrames
Use ODBC and JDBC drivers
AI solutions, generative AI,



Documentation Source:
cloud.google.com/bigquery/docs/samples/bigquery-query-destination-table.txt

Documentation Title:
Save query results  |  BigQuery  |  Google Cloud

Documentation Content:
encryption key
Create a job
Create a model
Create a regression model with BigQuery DataFrames
Create a routine
Create a routine with DDL
Create a table
Create a table using a template
Create a view
Create a view with DDL
Create an authorized view
Create an integer-range partitioned table
Create credentials with scopes
Create external table with hive partitioning
Create IAM policy
Create materialized view
Create table with schema
Delete a dataset
Delete a dataset and its contents
Delete a label from a dataset
Delete a label from a table
Delete a model
Delete a routine
Delete a table
Delete materialized view
Deploy and apply a remote function using BigQuery DataFrames
Disable query cache
Download public table data to DataFrame
Download public table data to DataFrame from the sandbox
Download query results to a GeoPandas GeoDataFrame
Download query results to DataFrame
Download table data to DataFrame
Dry run query
Enable large results
Export a model
Export a table to a compressed file
Export a table to a CSV file
Export a table to a JSON file
Generate text with the BigQuery DataFrames API
Get a model
Get a routine
Get dataset labels
Get dataset properties
Get job properties
Get table labels
Get table properties
Get view properties
Grant view access
Import a local file
Insert GeoJSON data
Insert rows with no IDs
Insert WKT data
List by label
List datasets
List jobs
List models
List models using streaming
List routines
List tables
Load a CSV file
Load a CSV file to replace a table
Load a CSV file with autodetect schema
Load a DataFrame to BigQuery with pandas-gbq
Load a JSON file
Load a JSON file to replace a table
Load a JSON file with autodetect schema
Load a Parquet file
Load a Parquet to replace a table
Load a table in JSON format
Load an Avro file
Load an Avro file to replace a table
Load an ORC file
Load an ORC file to replace a table
Load data from DataFrame
Load data into a column-based time partitioning table
Migration Guide: pandas-gbq
Migration Guide: pandas-gbq
Named parameters
Named parameters and provided types
Nested



Documentation Source:
cloud.google.com/bigquery/docs/remote-function-tutorial.txt

Documentation Title:
Tutorial: Analyze an object table by using a remote function  |  BigQuery  |  Google Cloud

Documentation Content:
Click
Save
.
Note:
There can be a delay of up to a minute before new permissions take effect.
Upload the dataset to Cloud Storage
Get the dataset files and make them available in Cloud Storage:
Download
the flowers dataset to your local machine.
Upload
the dataset to the bucket you
previously created.
Create an object table
Create an object table named
sample_images
based on the flowers dataset you
uploaded:
SQL
Go to the
BigQuery
page.
Go to BigQuery
In the
Editor
pane, run the following SQL statement:
CREATE EXTERNAL TABLE remote_function_test.sample_images
WITH CONNECTION `us.lake-connection`
OPTIONS(
  object_metadata = 'SIMPLE',
  uris = ['gs://
BUCKET_NAME
/*']);
Replace
BUCKET_NAME
with the name of the bucket you
previously created.
bq
In Cloud Shell, run the
bq mk
command
to create the connection:
bq mk --table \
--external_table_definition=gs:"//
BUCKET_NAME
/*@us.lake-connection" \
--object_metadata=SIMPLE \
remote_function_test.sample_images
Replace
BUCKET_NAME
with the name of the bucket you
previously created.
Create the BigQuery remote function
Create a remote function named
label_detection
:
Go to the
BigQuery
page.
Go to BigQuery
In the
Editor
pane, run the following SQL statement:
CREATE OR REPLACE FUNCTION `remote_function_test.label_detection` (signed_url_ STRING) RETURNS JSON
REMOTE WITH CONNECTION `us.lake-connection`
OPTIONS(
endpoint = '
TRIGGER_URL
',
max_batching_rows = 1
);
Replace
TRIGGER_URL
with the trigger URL that
you saved earlier. The URL should look similar to
https://vision-ai-1abcd2efgh-uc.a.run.app
.
Note:
When you specify the
CREATE FUNCTION
statement
for the remote function, we recommend that you set the
max_batching_rows
option to 1 in order to
avoid Cloud Functions timeout
and increase processing parallelism.



Documentation Source:
cloud.google.com/bigquery/docs/samples/bigquery-query-destination-table.txt

Documentation Title:
Save query results  |  BigQuery  |  Google Cloud

Documentation Content:
bigquery.query(queryConfig);

      // The results are now saved in the destination table.

      System.out.println("Saved query ran successfully");
    } catch (BigQueryException | InterruptedException e) {
      System.out.println("Saved query did not run \n" + e.toString());
    }
  }
}
Node.js
Before trying this sample, follow the
Node.js
setup instructions in the
BigQuery quickstart using
            client libraries
.
        
      
      
  For more information, see the
BigQuery
Node.js
API
    reference documentation
.
To authenticate to BigQuery, set up Application Default Credentials.
      For more information, see
Set up authentication for client libraries
.
// Import the Google Cloud client library
const {BigQuery} = require('@google-cloud/bigquery');
const bigquery = new BigQuery();

async function queryDestinationTable() {
  // Queries the U.S. given names dataset for the state of Texas
  // and saves results to permanent table.

  /**
   * TODO(developer): Uncomment the following lines before running the sample.
   */
  // const datasetId = 'my_dataset';
  // const tableId = 'my_table';

  // Create destination table reference
  const dataset = bigquery.dataset(datasetId);
  const destinationTable = dataset.table(tableId);

  const query = `SELECT name
    FROM \`bigquery-public-data.usa_names.usa_1910_2013\`
    WHERE state = 'TX'
    LIMIT 100`;

  // For all options, see https://cloud.google.com/bigquery/docs/reference/v2/tables#resource
  const options = {
    query: query,
    // Location must match that of the dataset(s) referenced in the query.
    location: 'US',
    destination: destinationTable,
  };

  // Run the query as a job
  const [job] = await bigquery.createQueryJob(options);

  console.log(`Job ${job.id} started.`);
  console.log(`Query results loaded to table ${destinationTable.id}`);
}
Python
Before trying this sample, follow the
Python
setup instructions in the
BigQuery quickstart using
            client libraries
.
        
      
      
  For more information, see the
BigQuery
Python
API
    reference documentation
.



