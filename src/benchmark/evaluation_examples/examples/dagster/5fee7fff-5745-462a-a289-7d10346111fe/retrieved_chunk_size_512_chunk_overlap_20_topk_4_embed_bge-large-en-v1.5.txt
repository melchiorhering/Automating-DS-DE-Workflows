Documentation Source:
release-1-7-2.dagster.dagster-docs.io/tutorial/building-an-asset-graph.txt

Documentation Title:
Tutorial, part four: Building an asset graph | Dagster Docs

Documentation Content:
Step 2: Creating an unstructured data asset
#
Along with structured data like tables, Dagster's assets can also be unstructured data, such as JSON files or images. Your next and final asset will take the DataFrame of stories and create a dictionary of the most frequent words in the titles.
Below is the finished code for a
most_frequent_words
asset. Copy and paste the code into
assets.py
:
@asset
(
deps
=
[
topstories
]
)
def
most_frequent_words
(
)
-
>
None
:
stopwords
=
[
"a"
,
"the"
,
"an"
,
"of"
,
"to"
,
"in"
,
"for"
,
"and"
,
"with"
,
"on"
,
"is"
]
topstories
=
pd
.
read_csv
(
"data/topstories.csv"
)
# loop through the titles and count the frequency of each word
word_counts
=
{
}
for
raw_title
in
topstories
[
"title"
]
:
title
=
raw_title
.
lower
(
)
for
word
in
title
.
split
(
)
:
cleaned_word
=
word
.
strip
(
".,-!?:;()[]'\"-"
)
if
cleaned_word
not
in
stopwords
and
len
(
cleaned_word
)
>
0
:
word_counts
[
cleaned_word
]
=
word_counts
.
get
(
cleaned_word
,
0
)
+
1
# Get the top 25 most frequent words
top_words
=
{
pair
[
0
]
:
pair
[
1
]
for
pair
in
sorted
(
word_counts
.
items
(
)
,
key
=
lambda
x
:
x
[
1
]
,
reverse
=
True
)
[
:
25
]
}
with
open
(
"data/most_frequent_words.json"
,
"w"
)
as
f
:
json
.
dump
(
top_words
,
f
)
Step 3: Educating users with metadata
#
Up until now, you've annotated your asset functions with
None
, meaning the asset doesn't return anything.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/tutorial/building-an-asset-graph.txt

Documentation Title:
Tutorial, part four: Building an asset graph | Dagster Docs

Documentation Content:
split
(
)
:
cleaned_word
=
word
.
strip
(
".,-!?:;()[]'\"-"
)
if
cleaned_word
not
in
stopwords
and
len
(
cleaned_word
)
>
0
:
word_counts
[
cleaned_word
]
=
word_counts
.
get
(
cleaned_word
,
0
)
+
1
# Get the top 25 most frequent words
top_words
=
{
pair
[
0
]
:
pair
[
1
]
for
pair
in
sorted
(
word_counts
.
items
(
)
,
key
=
lambda
x
:
x
[
1
]
,
reverse
=
True
)
[
:
25
]
}
# Make a bar chart of the top 25 words
plt
.
figure
(
figsize
=
(
10
,
6
)
)
plt
.
bar
(
list
(
top_words
.
keys
(
)
)
,
list
(
top_words
.
values
(
)
)
)
plt
.
xticks
(
rotation
=
45
,
ha
=
"right"
)
plt
.
title
(
"Top 25 Words in Hacker News Titles"
)
plt
.
tight_layout
(
)
# Convert the image to a saveable format
buffer
=
BytesIO
(
)
plt
.
savefig
(
buffer
,
format
=
"png"
)
image_data
=
base64
.
b64encode
(
buffer
.
getvalue
(
)
)
# Convert the image to Markdown to preview it within Dagster
md_content
=
f"![img](data:image/png;base64,
{
image_data
.
decode
(
)
}
)"
with
open
(
"data/most_frequent_words.json"
,
"w"
)
as
f
:
json
.
dump
(
top_words
,
f
)
# Attach the Markdown content as metadata to the asset
return
MaterializeResult
(
metadata
=
{
"plot"
:
MetadataValue
.
md
(
md_content
)
}
)
Reload your definitions and rematerialize your assets.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/getting-started/quickstart.txt

Documentation Title:
Quickstart | Dagster Docs

Documentation Content:
json
(
)
with
open
(
config
.
hn_top_story_ids_path
,
"w"
)
as
f
:
json
.
dump
(
top_story_ids
[
:
config
.
top_stories_limit
]
,
f
)
@asset
(
deps
=
[
hackernews_top_story_ids
]
)
def
hackernews_top_stories
(
config
:
HNStoriesConfig
)
-
>
MaterializeResult
:
"""Get items based on story ids from the HackerNews items endpoint."""
with
open
(
config
.
hn_top_story_ids_path
,
"r"
)
as
f
:
hackernews_top_story_ids
=
json
.
load
(
f
)
results
=
[
]
for
item_id
in
hackernews_top_story_ids
:
item
=
requests
.
get
(
f"https://hacker-news.firebaseio.com/v0/item/
{
item_id
}
.json"
)
.
json
(
)
results
.
append
(
item
)
df
=
pd
.
DataFrame
(
results
)
df
.
to_csv
(
config
.
hn_top_stories_path
)
return
MaterializeResult
(
metadata
=
{
"num_records"
:
len
(
df
)
,
"preview"
:
MetadataValue
.
md
(
str
(
df
[
[
"title"
,
"by"
,
"url"
]
]
.
to_markdown
(
)
)
)
,
}
)
Next steps
#
Congratulations on successfully running your first Dagster pipeline! In this example, we used
assets
, which are a cornerstone of Dagster projects. They empower data engineers to:
Think in the same terms as stakeholders
Answer questions about data quality and lineage
Work with the modern data stack (dbt, Airbyte/Fivetran, Spark)
Create declarative freshness policies instead of task-driven cron schedules
Dagster also offers
ops and jobs
, but we recommend starting with assets.
To create your own project, consider the following options:
Scaffold a new project using our
new project guide
.
Begin with an official example, like the
dbt & Dagster project
, and explore
all examples on GitHub
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/guides/dagster/ml-pipeline.txt

Documentation Title:
Building machine learning pipelines with Dagster | Dagster Docs

Documentation Content:
This will be a supervised model since we have the number of comments for all the previous stories.
The assets graph will look like this at the end of this guide (click to expand):
Ingesting data
#
First, we will create an asset that retrieves the most recent Hacker News records.
import
requests
from
dagster
import
asset
import
pandas
as
pd
@asset
def
hackernews_stories
(
)
:
# Get the max ID number from hacker news
latest_item
=
requests
.
get
(
"https://hacker-news.firebaseio.com/v0/maxitem.json"
)
.
json
(
)
# Get items based on story ids from the HackerNews items endpoint
results
=
[
]
scope
=
range
(
latest_item
-
1000
,
latest_item
)
for
item_id
in
scope
:
item
=
requests
.
get
(
f"https://hacker-news.firebaseio.com/v0/item/
{
item_id
}
.json"
)
.
json
(
)
results
.
append
(
item
)
# Store the results in a dataframe and filter on stories with valid titles
df
=
pd
.
DataFrame
(
results
)
if
len
(
df
)
>
0
:
df
=
df
[
df
.
type
==
"story"
]
df
=
df
[
~
df
.
title
.
isna
(
)
]
return
df
Transforming data
#
Now that we have a dataframe with all valid stories, we want to transform that data into something our machine learning model will be able to use.
The first step is taking the dataframe and splitting it into a
training and test set
. In some of your models, you also might choose to have an additional split for a validation set. The reason we split the data is so that we can have a test and/or a validation dataset that is independent of the training set. We can then use that dataset to see how well our model did.
from
sklearn
.



