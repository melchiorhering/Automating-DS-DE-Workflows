Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-json.txt

Documentation Title:
Loading JSON data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
Loading JSON data into a new table
To load JSON data from Cloud Storage into a new BigQuery
table:
Console
In the Google Cloud console, go to the
BigQuery
page.
Go to BigQuery
In the
Explorer
pane, expand your project, and then select a dataset.
In the
Dataset info
section, click
add_box
Create table
.
In the
Create table
panel, specify the following details:
In the
Source
section, select
Google Cloud Storage
in the
Create table from
list.
  Then, do the following:
Select a file from the Cloud Storage bucket, or enter the
Cloud Storage URI
.
              You cannot include multiple URIs
              in the Google Cloud console, but
wildcards
are supported. The Cloud Storage bucket must be in the same
              location as the dataset that contains the table you want to create, append, or
              overwrite.
For
File format
, select
JSONL (Newline delimited JSON)
.
In the
Destination
section, specify the following details:
For
Dataset
, select the dataset in which you want to create the
        table.
In the
Table
field, enter the name of the table that you want to create.
Verify that the
Table type
field is set to
Native table
.
In the
Schema
section, enter the
schema
definition.
   To enable the
auto detection
of a schema,
  select
Auto detect
.



  

  You can enter schema information manually by using one of
     the following methods:
Option 1: Click
Edit as text
and paste the schema in the form of a
        JSON array. When you use a JSON array, you generate the schema using the
        same process as
creating a JSON schema file
.
        You can view the schema of an existing table in JSON format by entering the following
      command:
bq show
--format=prettyjson
dataset.table
Option 2: Click
add_box
Add field
and enter the table schema. Specify each field's
Name
,
Type
,
        and
Mode
.
Optional: Specify
Partition and cluster settings
.



Documentation Source:
cloud.google.com/bigquery/docs/batch-loading-data.txt

Documentation Title:
Batch loading data  |  BigQuery  |  Google Cloud

Documentation Content:
Schema
information is self-described in the source data for other supported
file types.
You can also enter schema information manually by:
Clicking
Edit as text
and entering the table schema as a JSON
array:
Note:
You can view the schema of an existing table in JSON
format by entering the following command:
bq show --format=prettyjson
dataset.table
.
Using
Add Field
to manually input the schema.
Select applicable items in the
Advanced options
section For information on the available options, see
CSV options
and
JSON options
.
Optional: In the
Advanced options
choose the write disposition:
Write if empty
: Write the data only if the table is empty.
Append to table
: Append the data to the end of the table. This
setting is the default.
Overwrite table
: Erase all existing data in the table before
writing the new data.
Click
Create Table
.
bq
Use the
bq load
command, specify the
source_format
, and include the path
to the local file.
(Optional) Supply the
--location
flag and set the value to your
location
.
If you are loading data in a project other than your default project, add
the project ID to the dataset in the following format:
PROJECT_ID:DATASET
.
bq --location=
LOCATION
load \
--source_format=
FORMAT
\
PROJECT_ID:DATASET.TABLE
\
PATH_TO_SOURCE
\
SCHEMA
Replace the following:
LOCATION
: your location. The
--location
flag is
optional. For example, if you are using BigQuery in the
Tokyo region, set the flag's value to
asia-northeast1
. You can set a
default value for the location by using the
.bigqueryrc file
.
FORMAT
:
CSV
,
AVRO
,
PARQUET
,
ORC
, or
NEWLINE_DELIMITED_JSON
.
project_id
: your project ID.
dataset
: an existing dataset.
table
: the name of the table into which you're
loading data.
path_to_source
: the path to the local file.
schema
: a valid schema.



Documentation Source:
cloud.google.com/bigquery/docs/schemas.txt

Documentation Title:
Specifying a schema  |  BigQuery  |  Google Cloud

Documentation Content:
dataset
: the dataset that contains the table into
which you're loading data.
table
: the name of the table into which you're
loading data.
path_to_data_file
: the location of the CSV or JSON
data file on your local machine or in Cloud Storage.
path_to_schema_file
: the path to the schema file on
your local machine.
Example:
Enter the following command to load data from a local CSV file named
myfile.csv
into
mydataset.mytable
in your default project. The schema is
specified in
myschema.json
in the current directory.
bq load --source_format=CSV mydataset.mytable ./myfile.csv ./myschema.json
Python
Before trying this sample, follow the
Python
setup instructions in the
BigQuery quickstart using
            client libraries
.
        
      
      
  For more information, see the
BigQuery
Python
API
    reference documentation
.
To authenticate to BigQuery, set up Application Default Credentials.
      For more information, see
Set up authentication for client libraries
.
To load a table schema from a JSON file using the Python client library, call the
schema_from_json
method.
from google.cloud import bigquery

client = bigquery.Client()

# TODO(dev): Change uri variable to the path of your data file.
uri = "gs://your-bucket/path/to/your-file.csv"
# TODO(dev): Change table_id to the full name of the table you want to create.
table_id = "your-project.your_dataset.your_table"
# TODO(dev): Change schema_path variable to the path of your schema file.
schema_path = "path/to/schema.json"
# To load a schema file use the schema_from_json method.
schema = client.schema_from_json(schema_path)

job_config = bigquery.LoadJobConfig(
    # To use the schema you loaded pass it into the
    # LoadJobConfig constructor.
    schema=schema,
    skip_leading_rows=1,
)

# Pass the job_config object to the load_table_from_file,
# load_table_from_json, or load_table_from_uri method
# to use the schema on a new table.
load_job = client.load_table_from_uri(
    uri, table_id, job_config=job_config
)  # Make an API request.



Documentation Source:
cloud.google.com/bigquery/docs/loading-data-cloud-storage-json.txt

Documentation Title:
Loading JSON data from Cloud Storage  |  BigQuery  |  Google Cloud

Documentation Content:
You cannot include multiple URIs
              in the Google Cloud console, but
wildcards
are supported. The Cloud Storage bucket must be in the same
              location as the dataset that contains the table you want to create, append, or
              overwrite.
For
File format
, select
JSONL (Newline delimited JSON)
.
Note:
It is possible to modify the table's schema when you append or
      overwrite it. For more information about supported schema changes during a
      load operation, see
Modifying table schemas
.
In the
Destination
section, specify the following details:
For
Dataset
, select the dataset in which you want to create the
        table.
In the
Table
field, enter the name of the table that you want to create.
Verify that the
Table type
field is set to
Native table
.
In the
Schema
section, enter the
schema
definition.
   To enable the
auto detection
of a schema,
  select
Auto detect
.



  

  You can enter schema information manually by using one of
     the following methods:
Option 1: Click
Edit as text
and paste the schema in the form of a
        JSON array. When you use a JSON array, you generate the schema using the
        same process as
creating a JSON schema file
.
        You can view the schema of an existing table in JSON format by entering the following
      command:
bq show
--format=prettyjson
dataset.table
Option 2: Click
add_box
Add field
and enter the table schema. Specify each field's
Name
,
Type
,
        and
Mode
.
Note:
It is possible to modify the table's schema when you append or
      overwrite it. For more information about supported schema changes during a
      load operation, see
Modifying table schemas
.
Optional: Specify
Partition and cluster settings
. For more information, see
Creating partitioned tables
and
Creating and using clustered tables
.
     You
    cannot convert a table to a partitioned or clustered table by appending or
    overwriting it.



