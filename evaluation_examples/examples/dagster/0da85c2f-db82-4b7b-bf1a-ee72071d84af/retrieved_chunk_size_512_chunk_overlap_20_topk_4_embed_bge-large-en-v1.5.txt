Documentation Source:
release-1-7-2.dagster.dagster-docs.io/guides/dagster/managing-ml.txt

Documentation Title:
Managing machine learning models with Dagster | Dagster Docs

Documentation Content:
.
.
@asset
(
auto_materialize_policy
=
AutoMaterializePolicy
.
lazy
(
)
)
def
some_ml_model
(
some_data
)
:
.
.
.
@asset
(
auto_materialize_policy
=
AutoMaterializePolicy
.
lazy
(
)
,
freshness_policy
=
FreshnessPolicy
(
maximum_lag_minutes
=
7
*
24
*
60
)
,
)
def
predictions
(
some_ml_model
)
:
.
.
.
A more traditional schedule can also be used to update machine learning assets, causing them to be re-materialized or retrained on the latest data. For example, setting up a
cron schedule on a daily basis
.
This can be useful if you have data that is also being scheduled on a cron schedule and want to add your machine model jobs to run on a schedule as well.
from
dagster
import
AssetSelection
,
define_asset_job
,
ScheduleDefinition

ml_asset_job
=
define_asset_job
(
"ml_asset_job"
,
AssetSelection
.
groups
(
"ml_asset_group"
)
)
basic_schedule
=
ScheduleDefinition
(
job
=
ml_asset_job
,
cron_schedule
=
"0 9 * * *"
)
Monitoring
#
Integrating your machine learning models into Dagster allows you to see when the model and its data dependencies were refreshed, or when a refresh process has failed. By using Dagster to monitor performance changes and process failures on your ML model, it becomes possible to set up remediation paths, such as automated model retraining, that can help resolve issues like model drift.
In this example, the model is being evaluated against the previous model’s accuracy. If the model’s accuracy has improved, the model is returned for use in downstream steps, such as inference or deploying to production.
from
sklearn
import
linear_model
from
dagster
import
asset
,
Output
,
AssetKey
,
AssetExecutionContext
import
numpy
as
np
from
sklearn
.
model_selection
import
train_test_split
@asset
(
output_required
=
False
)
def
conditional_machine_learning_model
(
context
:
AssetExecutionContext
)
:
X
,
y
=
np
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/guides/dagster/ml-pipeline.txt

Documentation Title:
Building machine learning pipelines with Dagster | Dagster Docs

Documentation Content:
This will be a supervised model since we have the number of comments for all the previous stories.
The assets graph will look like this at the end of this guide (click to expand):
Ingesting data
#
First, we will create an asset that retrieves the most recent Hacker News records.
import
requests
from
dagster
import
asset
import
pandas
as
pd
@asset
def
hackernews_stories
(
)
:
# Get the max ID number from hacker news
latest_item
=
requests
.
get
(
"https://hacker-news.firebaseio.com/v0/maxitem.json"
)
.
json
(
)
# Get items based on story ids from the HackerNews items endpoint
results
=
[
]
scope
=
range
(
latest_item
-
1000
,
latest_item
)
for
item_id
in
scope
:
item
=
requests
.
get
(
f"https://hacker-news.firebaseio.com/v0/item/
{
item_id
}
.json"
)
.
json
(
)
results
.
append
(
item
)
# Store the results in a dataframe and filter on stories with valid titles
df
=
pd
.
DataFrame
(
results
)
if
len
(
df
)
>
0
:
df
=
df
[
df
.
type
==
"story"
]
df
=
df
[
~
df
.
title
.
isna
(
)
]
return
df
Transforming data
#
Now that we have a dataframe with all valid stories, we want to transform that data into something our machine learning model will be able to use.
The first step is taking the dataframe and splitting it into a
training and test set
. In some of your models, you also might choose to have an additional split for a validation set. The reason we split the data is so that we can have a test and/or a validation dataset that is independent of the training set. We can then use that dataset to see how well our model did.
from
sklearn
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/guides/dagster/ml-pipeline.txt

Documentation Title:
Building machine learning pipelines with Dagster | Dagster Docs

Documentation Content:
fillna
(
0
)
transformed_y_train
=
np
.
array
(
y_train
)
return
vectorizer
,
(
transformed_X_train
,
transformed_y_train
)
@asset
def
transformed_test_data
(
test_data
,
tfidf_vectorizer
)
:
X_test
,
y_test
=
test_data
# Use the fitted tokenizer to transform the test dataset
transformed_X_test
=
tfidf_vectorizer
.
transform
(
X_test
)
transformed_y_test
=
np
.
array
(
y_test
)
y_test
=
y_test
.
fillna
(
0
)
transformed_y_test
=
np
.
array
(
y_test
)
return
transformed_X_test
,
transformed_y_test
We also transformed the dataframes into NumPy arrays and removed
nan
values to prepare the data for training.
Training the model
#
At this point, we have
X_train
,
y_train
,
X_test
, and
y_test
ready to go for our model. To train our model, we can use any number of models from libraries like
sklearn
,
TensorFlow
, and
PyTorch
.
In our example, we will train an
XGBoost model
to predict a numerical value.
import
xgboost
as
xg
from
sklearn
.
metrics
import
mean_absolute_error
@asset
def
xgboost_comments_model
(
transformed_training_data
)
:
transformed_X_train
,
transformed_y_train
=
transformed_training_data
# Train XGBoost model, which is a highly efficient and flexible model
xgb_r
=
xg
.
XGBRegressor
(
objective
=
"reg:squarederror"
,
eval_metric
=
mean_absolute_error
,
n_estimators
=
20
)
xgb_r
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/guides/dagster/ml-pipeline.txt

Documentation Title:
Building machine learning pipelines with Dagster | Dagster Docs

Documentation Content:
We can then use that dataset to see how well our model did.
from
sklearn
.
model_selection
import
train_test_split
from
dagster
import
multi_asset
,
AssetOut
@multi_asset
(
outs
=
{
"training_data"
:
AssetOut
(
)
,
"test_data"
:
AssetOut
(
)
}
)
def
training_test_data
(
hackernews_stories
)
:
X
=
hackernews_stories
.
title
    y
=
hackernews_stories
.
descendants
# Split the dataset to reserve 20% of records as the test set
X_train
,
X_test
,
y_train
,
y_test
=
train_test_split
(
X
,
y
,
test_size
=
0.2
)
return
(
X_train
,
y_train
)
,
(
X_test
,
y_test
)
Next, we will take both the training and test data subsets and
tokenize the titles
e.g. take the words and turn them into columns with the frequency of terms for each record to create
features
for the data. To do this, we will be using the training set to fit the tokenizer. In this case, we are using
TfidfVectorizer
and then transforming both the training and test set based on that tokenizer.
from
sklearn
.
feature_extraction
.
text
import
TfidfVectorizer
import
numpy
as
np
@multi_asset
(
outs
=
{
"tfidf_vectorizer"
:
AssetOut
(
)
,
"transformed_training_data"
:
AssetOut
(
)
}
)
def
transformed_train_data
(
training_data
)
:
X_train
,
y_train
=
training_data
# Initiate and fit the tokenizer on the training data and transform the training dataset
vectorizer
=
TfidfVectorizer
(
)
transformed_X_train
=
vectorizer
.
fit_transform
(
X_train
)
transformed_X_train
=
transformed_X_train
.
toarray
(
)
y_train
=
y_train
.
fillna
(
0
)
transformed_y_train
=
np
.



