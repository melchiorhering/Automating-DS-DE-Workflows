Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers.txt

Documentation Title:
I/O managers | Dagster

Documentation Content:
Series
,
second_asset
:
pd
.
Series
)
-
>
pd
.
Series
:
return
pd
.
concat
(
[
first_asset
,
second_asset
,
pd
.
Series
(
[
7
,
8
]
)
]
)
defs
=
Definitions
(
assets
=
[
first_asset
,
second_asset
,
third_asset
]
,
resources
=
{
"pandas_series"
:
PandasSeriesIOManager
(
)
,
}
,
)
Using I/O managers with non-asset jobs
#
Job-wide I/O manager
#
By default, all the inputs and outputs in a job use the same I/O manager. This I/O manager is determined by the
ResourceDefinition
provided for the
"io_manager"
resource key.
"io_manager"
is a resource key that Dagster reserves specifically for this purpose.
Hereâ€™s how to specify that all op outputs are stored using the
FilesystemIOManager
, which pickles outputs and stores them on the local filesystem. It stores files in a directory with the run ID in the path, so that outputs from prior runs will never be overwritten.
from
dagster
import
FilesystemIOManager
,
job
,
op
@op
def
op_1
(
)
:
return
1
@op
def
op_2
(
a
)
:
return
a
+
1
@job
(
resource_defs
=
{
"io_manager"
:
FilesystemIOManager
(
)
}
)
def
my_job
(
)
:
op_2
(
op_1
(
)
)
Per-output I/O manager
#
Not all the outputs in a job should necessarily be stored the same way. Maybe some of the outputs should live on the filesystem so they can be inspected, and others can be transiently stored in memory.
To select the I/O manager for a particular output, you can set an
io_manager_key
on
Out
, and then refer to that
io_manager_key
when setting I/O managers in your job. In this example, the output of
op_1
will go to
FilesystemIOManager
and the output of
op_2
will go to
S3PickleIOManager
.
from
dagster_aws
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers-legacy.txt

Documentation Title:
IO Managers (Legacy) | Dagster

Documentation Content:
storage_dict
[
(
"123"
,
"abc"
)
]
=
5
context
=
build_input_context
(
upstream_output
=
build_output_context
(
name
=
"abc"
,
step_key
=
"123"
)
)
assert
manager
.
load_input
(
context
)
==
5
Recording metadata from an IO Manager
#
Sometimes, you may want to record some metadata while handling an output in an IO manager. To do this, you can invoke
OutputContext.add_output_metadata
from within the body of the
handle_output
function. Using this, we can modify one of the
above examples
to now include some helpful metadata in the log:
class
DataframeTableIOManagerWithMetadata
(
IOManager
)
:
def
handle_output
(
self
,
context
,
obj
)
:
table_name
=
context
.
name
        write_dataframe_to_table
(
name
=
table_name
,
dataframe
=
obj
)
context
.
add_output_metadata
(
{
"num_rows"
:
len
(
obj
)
,
"table_name"
:
table_name
}
)
def
load_input
(
self
,
context
)
:
table_name
=
context
.
upstream_output
.
name
return
read_dataframe_from_table
(
name
=
table_name
)
Any entries yielded this way will be attached to the
Handled Output
event for this output.
Additionally, if the handled output is part of a software-defined asset, these metadata entries will also be attached to the materialization event created for that asset and show up on the Asset Details page for the asset.
See it in action
#
For more examples of IO Managers, check out the following in our
Hacker News example
:
Parquet IO Manager
Our
Type and Metadata example
also covers writing custom IO managers.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers.txt

Documentation Title:
I/O managers | Dagster

Documentation Content:
root_path
+
"/"
.
join
(
asset_key
.
path
)
def
handle_output
(
self
,
context
:
OutputContext
,
obj
)
:
write_csv
(
self
.
_get_path
(
context
.
asset_key
)
,
obj
)
def
load_input
(
self
,
context
:
InputContext
)
:
return
read_csv
(
self
.
_get_path
(
context
.
asset_key
)
)
defs
=
Definitions
(
assets
=
.
.
.
,
resources
=
{
"io_manager"
:
MyIOManager
(
root_path
=
"/tmp/"
)
}
,
)
Handling partitioned assets
#
I/O managers can be written to handle
partitioned
assets. For a partitioned asset, each invocation of
handle_output
will (over)write a single partition, and each invocation of
load_input
will load one or more partitions. When the I/O manager is backed by a filesystem or object store, then each partition will typically correspond to a file or object. When it's backed by a database, then each partition will typically correspond to a range of rows in a table that fall within a particular window.
The default I/O manager has support for loading a partitioned upstream asset for a downstream asset with matching partitions out of the box (see the section below for loading multiple partitions). The
UPathIOManager
can be used to handle partitions in custom filesystem-based I/O managers.
To handle partitions in an custom I/O manager, you'll need to determine which partition you're dealing with when you're storing an output or loading an input. For this,
OutputContext
and
InputContext
have a
asset_partition_key
property:
class
MyPartitionedIOManager
(
IOManager
)
:
def
_get_path
(
self
,
context
)
-
>
str
:
if
context
.
has_partition_key
:
return
"/"
.
join
(
context
.
asset_key
.
path
+
[
context
.
asset_partition_key
]
)
else
:
return
"/"
.
join
(
context
.
asset_key
.



Documentation Source:
release-1-7-2.dagster.dagster-docs.io/concepts/io-management/io-managers.txt

Documentation Title:
I/O managers | Dagster

Documentation Content:
Here are some examples:
import
pandas
as
pd
from
upath
import
UPath
from
dagster
import
(
InputContext
,
OutputContext
,
UPathIOManager
,
)
class
PandasParquetIOManager
(
UPathIOManager
)
:
extension
:
str
=
".parquet"
def
dump_to_path
(
self
,
context
:
OutputContext
,
obj
:
pd
.
DataFrame
,
path
:
UPath
)
:
with
path
.
open
(
"wb"
)
as
file
:
obj
.
to_parquet
(
file
)
def
load_from_path
(
self
,
context
:
InputContext
,
path
:
UPath
)
-
>
pd
.
DataFrame
:
with
path
.
open
(
"rb"
)
as
file
:
return
pd
.
read_parquet
(
file
)
The extension attribute defines the suffix all the file paths generated by the IOManager will end with.
The I/O managers defined above will work with partitioned assets on any filesystem:
from
typing
import
Optional
from
dagster
import
ConfigurableIOManagerFactory
,
EnvVar
class
LocalPandasParquetIOManager
(
ConfigurableIOManagerFactory
)
:
base_path
:
Optional
[
str
]
=
None
def
create_io_manager
(
self
,
context
)
-
>
PandasParquetIOManager
:
base_path
=
UPath
(
self
.
base_path
or
context
.
instance
.
storage_directory
(
)
)
return
PandasParquetIOManager
(
base_path
=
base_path
)
class
S3ParquetIOManager
(
ConfigurableIOManagerFactory
)
:
base_path
:
str
aws_access_key
:
str
=
EnvVar
(
"AWS_ACCESS_KEY_ID"
)
aws_secret_key
:
str
=
EnvVar
(
"AWS_SECRET_ACCESS_KEY"
)
def
create_io_manager
(
self
,
context
)
-
>
PandasParquetIOManager
:
base_path
=
UPath
(
self
.
base_path
)
assert
str
(
base_path
)
.



