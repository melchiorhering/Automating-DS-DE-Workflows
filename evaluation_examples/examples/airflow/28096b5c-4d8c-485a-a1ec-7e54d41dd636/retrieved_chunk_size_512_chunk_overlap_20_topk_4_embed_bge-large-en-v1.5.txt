Documentation Source:
docs.astronomer.io/astro/cli/configure-cli.txt

Documentation Title:
Configure the Astro CLI | Astronomer Documentation

Documentation Content:
true
true
or
false
local.registry
The location of your local Docker container running Airflow.
localhost:5555
Any available port
postgres.user
The username for the Postgres metadata database.
postgres
Any string
postgres.password
The password for the Postgres metadata database.
postgres
Any string
postgres.host
The hostname for the Postgres metadata database.
postgres
Any string
postgres.port
The port for the Postgres metadata database.
5432
Any available port
postgres.repository
Image repository to pull the Postgres image from
docker.io/postgres
Any Postgres image in a repository
postgres.tag
The tag for your Postgres image
12.6
Any valid image tag
project.name
The name of your Astro project.
Empty string
Any string
show_warnings
Determines whether warning messages appear when starting a local Airflow environment. For example, when set to
true
, you'll receive warnings when a new version of Astro Runtime is available and when your Astro project doesn't have any DAGs.
true
true
,
false
skip_parse
Determines whether the CLI parses DAGs before pushing code to a Deployment.
false
true
,
false
upgrade_message
Determines whether a message indicating the availability of a new Astro CLI version displays in the Astro CLI.
true
true
,
false
webserver.port
The port for the webserver in your local Airflow environment.
8080
Any available port
Option
Description
Default value
Valid values
houston.dial_timeout
The time in seconds to wait for a Houston connection.
10
Any integer
houston.skip_verify_tls
Determines whether the Transport Layer Security (TLS) certificate is verified when connecting to Houston.
false
true
,
false
interactive
Determines whether responses are paginated in the Astro CLI when pagination is supported.
false
true
,
false
page_size
Determines the size of the paginated response when
interactive
is set to
true
.
20
Any integer
postgres.user
The username for the Postgres metadata database.
postgres
Any string
postgres.password
The password for the Postgres metadata database.
postgres
Any string
postgres.host
The hostname for the Postgres metadata database.
postgres
Any string
postgres.port
The port for the Postgres metadata database.



Documentation Source:
docs.astronomer.io/astro/cli/run-airflow-locally.txt

Documentation Title:
Run your Astro project in a local Airflow environment with the CLI | Astronomer Documentation

Documentation Content:
Make requests to the Airflow REST API locally
​
Make requests to the
Airflow REST API
in a local Airflow environment with HTTP basic access authentication. This can be useful for testing and troubleshooting API calls before executing them in a Deployment on Astro.
To make local requests with cURL or Python, you only need the username and password for your local user. Both of these values are
admin
by default. They are the same credentials for logging into the Airflow UI, and they're listed when you run
astro dev start
.
To make requests to the Airflow REST API in a Deployment on Astro, see
Airflow API
.
cURL
​
curl
-X
GET localhost:8080/api/v1/
<
endpoint
>
--user
"admin:admin"
Python
​
import
requests
response
=
requests
.
get
(
url
=
"http://localhost:8080/api/v1/<endpoint>"
,
auth
=
(
"admin"
,
"admin"
)
)
Hard reset your local environment
​
In most cases,
restarting your local project
is sufficient for testing and making changes to your project. However, it is sometimes necessary to kill your Docker containers and metadata database for testing purposes. To do so, run the following command:
astro dev
kill
This command forces your running containers to stop and deletes all data associated with your local Postgres metadata database, including Airflow connections, logs, and task history.
Override the Astro CLI Docker Compose file
​
The Astro CLI uses a default set of
Docker Compose
configurations to define and run local Airflow components. For advanced testing cases, you might need to override these default configurations. For example, you might need to:
Add extra containers to mimic services that your Airflow environment needs to interact with locally, such as an SFTP server.
Change the volumes mounted to any of your local containers.
info
The Astro CLI does not support overrides to environment variables that are required globally. For the list of environment variables that Astro enforces, see
Global environment variables
. To learn more about environment variables, read
Environment variables
.



Documentation Source:
docs.astronomer.io/learn/airflow-duckdb.txt

Documentation Title:
Use DuckDB with Apache Airflow | Astronomer Documentation

Documentation Content:
The Astro Python SDK is the ideal tool if you want to easily connect to several database tools without changing any underlying code.
In this tutorial we will cover the first two ways. To learn more about how to connect to DuckDB (and other data warehouses) with the Astro Python SDK, see
Write a DAG with the Astro Python SDK
.
Other ways to learn
There are multiple resources for learning about this topic. See also:
Webinar:
How to use DuckDB with Airflow
.
Example repository:
Astronomer's DuckDB example repository
.
Time to complete
​
This tutorial takes approximately 15 minutes to complete.
Assumed knowledge
​
To get the most out of this tutorial, make sure you have an understanding of:
The basics of DuckDB. See
the DuckDB documentation
.
Airflow fundamentals, such as writing DAGs and defining tasks. See
Get started with Apache Airflow
.
Airflow decorators. See
Introduction to Airflow decorators
.
Airflow connections. See
Manage connections in Apache Airflow
.
Prerequisites
​
The
Astro CLI
.
Step 1: Configure your Astro project
​
To use DuckDB with Airflow, install the
DuckDB Airflow provider
in your Astro project. This will also install the newest version of the
DuckDB Python package
.
Create a new Astro project:
$
mkdir
astro-duckdb-tutorial
&&
cd
astro-duckdb-tutorial
$ astro dev init
Add the DuckDB Airflow provider to your Astro project
requirements.txt
file.
airflow-provider-duckdb==0.2.0
If you are connecting to MotherDuck, the DuckDB cloud service, you need to use the amd64 version of Astro Runtime to prevent package conflicts. In this case, replace the
FROM
statement in your Dockerfile with the following line:
FROM
--platform
=
linux/amd64
quay.io/astronomer/astro-runtime:8.6.0
If you are only using DuckDB locally, you do not need to modify your Dockerfile.



Documentation Source:
docs.astronomer.io/learn/connections/postgres.txt

Documentation Title:
Create a Postgres connection in Airflow | Astronomer Documentation

Documentation Content:
Refer to the following documents to for more information about retrieveing these values:
AWS: Connect to Postgres running on
RDS
GCP: Connect to Postgres running on
Cloud SQL
Azure: Connect to Postgres running on an
Azure database
For example, if you're running Postgres in a Relational Data Store (RDS) in AWS, complete the following steps to retrieve these values:
In your AWS console, select your region, then go to the RDS service and select your Postgres database.
Open the
Connectivity & security
tab and copy the
Endpoint
and
Port
.
Follow the AWS instructions to
create a user
and
grant a role to the user
that Airflow will use to connect to Postgres. Copy the username and password.
(Optional) To use a specific schema, copy the name of the schema. If you skip this, the default schema
public
will be used.
Create your connection
​
Open your Astro project and add the following line to your
requirements.txt
file:
apache-airflow-providers-postgres
This will install the Postgres provider package, which makes the Postgres connection type available in Airflow.
Run
astro dev restart
to restart your local Airflow environment and apply your changes in
requirements.txt
.
In the Airflow UI for your local Airflow environment, go to
Admin
>
Connections
. Click
+
to add a new connection, then choose
Postgres
as the connection type.
Fill out the following connection fields using the information you retrieved from
Get connection details
:
Connection Id
: Enter a name for the connection.
Host
: Enter your Postgres server's host/ endpoint URL/ server name/ instance ID.
Schema
: Enter your schema name.
Login
: Enter your username.
Password
: Enter your password.
Port
: Enter your Postgres server's
Port
.
Click
Test
. After the connection test succeeds, click
Save
.
How it works
​
Airflow uses the
psycopg2
python library to connect to Postgres through the
PostgresHook
. You can also directly use the PostgresHook to create your own custom operators.



