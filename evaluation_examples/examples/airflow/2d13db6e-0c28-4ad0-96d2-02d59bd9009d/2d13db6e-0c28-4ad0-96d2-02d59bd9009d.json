{
    "id": "2d13db6e-0c28-4ad0-96d2-02d59bd9009d",
    "snapshot": "airflow",
    "instruction": "I have defined two DAGs to fetch and process data from TheCocktailDB. Currently, these two DAGs are configured to run at a fixed timestamp each day. However, since the consumer DAG is totally dependent on the output of the producer, I hope to change the schedule of the consumer DAG such that each time the resulting files of the producer are updated, the consumer DAG is triggered. Can you help me with this data-aware scheduling?\nHere is a step-by-step tutorial from an expert instructing you how to complete it:\nIn this task, we want to change the schedule of the consumer DAG `cocktail_consumer_dag`, such that each time the output files of the producer DAG `cocktail_producer_dag` are updated, `cocktail_consumer_dag` be be automatically triggered. We can implement this via airflow Datasets. The detailed steps are as follows:\n1. Click the application icon for Visual Studio Code on the left menu.\n2. Open the file `dags/cocktail_producer_dag.py`.\n3. Firstly, define two Datasets with respect to the output files `include/cocktail_instructions.txt` and `include/cocktail_info.txt`:\n```\n... previous import statements ...\n\nfrom airflow.datasets import Dataset # use Dataset\n\nINSTRUCTIONS = Dataset(\"file://localhost/airflow/include/cocktail_instructions.txt\")\nINFO = Dataset(\"file://localhost/airflow/include/cocktail_info.txt\")\n\nAPI = \"https://www.thecocktaildb.com/api/json/v1/1/random.php\"\n... other code ...\n```\n4. Next, add the `outlets` parameter for each write task in `cocktail_producer_dag`:\n```\ndef cocktail_producer_dag():\n    @task\n    def get_cocktail(api):\n        ...\n\n    @task(outlets=[INSTRUCTIONS])\n    def write_instructions_to_file(response):\n        ...\n\n    @task(outlets=[INFO])\n    def write_info_to_file(response):\n        ...\n```\n5. Save this file via hotkey Ctrl+S.\n6. Open the consumer DAG file `dags/cocktail_consumer_dag.py`.\n7. Revise the code by integrating two datasets into the schedule:\n```\n... previous import statements ...\n\nfrom airflow.datasets import Dataset\n\nINSTRUCTIONS = Dataset(\"file://localhost/airflow/include/cocktail_instructions.txt\")\nINFO = Dataset(\"file://localhost/airflow/include/cocktail_info.txt\")\n\n@dag(\n    dag_id=\"cocktail_consumer_dag\",\n    start_date=datetime(2024, 1, 1),\n    schedule=[INSTRUCTIONS, INFO],  # Scheduled on both Datasets\n    catchup=False,\n)\ndef cocktail_consumer_dag():\n    ...\n```\n8. Save this file too.\n9. Switch to the web browser.\n10. Refresh the web page via \"Ctrl+R\". We can see that the value under the column \"Schedule\" for DAG `cocktail_consumer_dag` has changed to \"Dataset\". This means that each time the defined two datasets are updated (namely the two output files of the producer DAG), the consumer DAG will be automatically triggered.\n(There may exist some time delay in step 10, we can refresh the web page multiple times and wait for the changes to work.)\nYou can exactly follow the detailed plan above or proactively tackle the task based on the real-time environment interaction by yourself.",
    "source": [
        "https://docs.astronomer.io/learn/airflow-datasets"
    ],
    "config": [
        {
            "type": "copyfile_from_host_to_guest",
            "parameters": {
                "src": "evaluation_examples/examples/airflow/2d13db6e-0c28-4ad0-96d2-02d59bd9009d/cocktail.zip",
                "dest": "/home/user/cocktail.zip"
            }
        },
        {
            "type": "script_and_execute",
            "parameters": {
                "src": "evaluation_examples/examples/airflow/2d13db6e-0c28-4ad0-96d2-02d59bd9009d/init.sh",
                "dest": "/home/user/init.sh"
            }
        },
        {
            "type": "google_chrome_browser",
            "parameters": {
                "debugging_port": 1337,
                "listening_port": 9222,
                "urls": [
                    "https://www.bing.com/"
                ]
            }
        },
        {
            "type": "astro_webui_init",
            "parameters": {
                "url": "http://localhost:8080",
                "listening_port": 9222,
                "actions": [
                    {
                        "type": "login",
                        "username": "admin",
                        "password": "admin"
                    }
                ]
            }
        }
    ],
    "related_apps": [
        "airflow",
        "docker",
        "vscode",
        "chromium"
    ],
    "tags": [
        "cli+gui",
        "data_orchestration",
        "verbose"
    ],
    "action_number": 10,
    "evaluator": {
        "func": "check_include_exclude",
        "result": {
            "type": "vm_script_output",
            "src": "evaluation_examples/examples/airflow/2d13db6e-0c28-4ad0-96d2-02d59bd9009d/eval.sh",
            "dest": "/home/user/eval.sh"
        },
        "expected": {
            "type": "rule",
            "rules": {
                "include": [
                    "succeed"
                ],
                "exclude": [
                    "failed"
                ]
            }
        }
    },
    "counterpart": "bdbefc1d-26d1-4486-99da-3ff5dd299b8d"
}