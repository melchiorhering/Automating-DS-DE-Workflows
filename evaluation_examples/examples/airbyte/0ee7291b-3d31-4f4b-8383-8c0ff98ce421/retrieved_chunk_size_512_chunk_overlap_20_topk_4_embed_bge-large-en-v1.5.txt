Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.txt

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
I’ll also be using `external_user` for my user, the `validator_wh` for my warehouse, and the `validator’ role here. These are different from what I used in the loading process since this is a different step in the data pipeline.
Make sure whatever role you are using has the appropriate permissions to access the table you are comparing! The user you are inputting in the string also has to be assigned the role you specified.
My final connection string looks like so:
"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&role=validator"
For Postgres, the connection string will be formatted like so:
postgresql://[username]:[password]@localhost:5432/[database]
I’m using the username and password that I typically use to connect to my Postgres database. Unfortunately, this is a personal username and password which is not a best practice. It’s always best to use a user that is created for the use of external tools like data-diff. This helps keep your databases secure and ensures you are always in tight control of access.
My final connection string looks like so:
postgresql://madison:[password]@localhost:5432/development
Now that you’ve built your connection strings, you can use them in the data-diff command, which looks like this:
data-diff DB1_URI TABLE1_NAME DB2_URI TABLE2_NAME
You will input Snowflake’s connection string as DB1_URI and Postgres’s as DB2_URI.
Although I’m choosing to do Snowflake first and Postges second, the order doesn’t matter. Then be sure to write the appropriate table names for each of the sources that you are comparing. Note that data-diff only allows you to compare two tables at one time rather than all of your tables in a schema. However, you could easily create a Python script using
data-diff Python’s SDK
that loops through a list of table names, running the command for every pair of tables specified and outputting the results in a nicely formatted CSV file.
Let’s start by comparing the `customer_contacts` table in our customers schemas.



Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.txt

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
This will allow you to easily compare the differences between the source table and the destination table. Your data will be replicated to the database “development” and the schema “customers”, with the same table names. If the naming convention were different, it may get confusing down the line.
When you choose the streams you wish to activate, be sure that you are selecting “full refresh | overwrite”. This will capture any deletes or updates in your source data table through the replication process.
Select “Normalized tabular data” and create your connection. Once you set up your connection, you should see your data syncing.
‍
When your connection is finished syncing and reads “successful”, you are ready to begin the validation process!
Validate the replication process with data-diff
Now that you have your Postgres database replicated to Snowflake, you want to validate that your data looks as expected. Normally you would have to go through a long process of performing different discovery queries on each data source and comparing values. And, unless you looked at every single row and did a row-by-row comparison, you wouldn’t be confident that your data replicated as expected. This is where data-diff comes in. Let’s walk through how to set this up.
First, you’ll want to install the open-source tool. You can do this with the following command:
pip install data-diff
Since you are using both Postgres and Snowflake, you want to install the drivers for these specific databases:
pip install 'data-diff[postgresql]'
pip install 'data-diff[snowflake]'
After you’ve installed data-diff, you’ll have to focus on creating your connection strings. These contain key information about the database you are connecting to like the database user, password, host, account, database, schema, warehouse, and role.
For Snowflake, your connection string will follow the following format:
"snowflake://[username]:[password]@[account]/[DATABASE]/[SCHEMA]?warehouse=[WAREHOUSE]&role=[ROLE]"
Because the table I replicated into Snowflake is in the `development` database and `customers` schema, I’ll input this information into the string.



Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.txt

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
Like this:
data-diff \
"snowflake://external_user:[password]@[password]/development/customers?warehouse=validator_wh&role=validator" customer_contacts \
"postgresql://madison:[password]@localhost:5432/development" customer_contacts \
-columns customer_contact_id customer_name phone_number email_address updated_at \
-key-columns customer_contact_id
And, lastly, if you ever want to filter the column values that you are comparing, you can specify a `where` (or just `w`) flag in the command. This acts as a where clause for your query, allowing you to filter for certain conditions. This is particularly helpful if you have a fast-changing source and need to validate a new batch of data that has been ingested after a previous validation check. Using this will prevent false positives of rows that have been updated since being ingested when checking for differing rows. We can add this to our command like so:
data-diff \
"snowflake://external_user:[password]@[account]/development/customers?warehouse=validator_wh&role=validator" customer_contacts \
"postgresql://madison:[password]@localhost:5432/development" customer_contacts \
-columns customer_contact_id customer_name phone_number email_address updated_at \
-key-columns customer_contact_id \
-where “updated_at <= '10-31-2022'"
Now we are ready to run data-diff!
Understanding data-diff output
Once you run the command, data-diff will give you the primary key values of the rows in the tables that differ. Each value will have a + or - in front of it. A + signifies that the rows exists in the second table but not the first. A - sign indicates that the row exists in the first table but not the second. Now, you can dig deeper into your validation process and explore
why
these primary keys are in one database and not the other.
Also, note that one primary key can be present with a + and a - sign in the output. This means that the row
exists
in both tables but has a column value that varies.



Documentation Source:
airbyte.com/tutorials/validate-data-replication-postgres-snowflake.txt

Documentation Title:
Validate data replication pipelines from Postgres to Snowflake with data-diff | Airbyte

Documentation Content:
First, we are going to start by setting up our Postgres source and Snowflake destination within Airbyte.
Set up your Postgres source
On
Airbyte Cloud
or
Airbyte Open Source
, click “new connection”. This will bring you to a screen where you can select your data source. Choose “
Postgres
” as your source type.
Now you will be brought to a screen where you need to enter some specific information about your Postgres database. This includes host, port, database name, and a list of the schemas you wish to sync.
I kept the default port and added my database named `development`, `customers` schema, and the login information for my Airbyte user. It is best practice to
create users specific to the tools
you are connecting to your database.
Set up your Snowflake destination
Now let’s set up our
Snowflake destination
where we will be replicating our Postgres data to. Start by clicking on “new destination” in the top right corner. Then select “Snowflake” as your destination type.
‍
This is where you will input the information for the Snowflake database that you are copying your Postgres data. Make sure you enter the right location information!
I also recommend setting up a role that is specific for loading data in your destination as well. This will help keep your environment secure and all you to closely monitor different metrics on the replication process.
Set up a Postgres to Snowflake connection
Now that you’ve created both your source in Postgres and your destination in Snowflake, you can set up a connection between the two to replicate your data from Postgres. Select “connections” on the left panel.
Select your Postgres source you created from the dropdown, then select Snowflake as your destination.
Now you’ll want to give the connection a good name and choose how often it replicates. I’m going to call mine “postgres_snowflake_replication” and set it t replicate every 24 hours.
I also recommend selecting “mirror source structure” for the “destination namespace”. This will allow you to easily compare the differences between the source table and the destination table.



