{
    "id": "d520508c-9b4a-4292-a9cd-0d65d6a2244e",
    "snapshot": "airbyte",
    "instruction": "I have created an airbyte connection from Faker to Local JSON. Could you help me use Airflow to trigger this synchronization and perform extra processing steps? Please name the target DAG as `faker_to_json_dag` and see the opened vscode project for detailed requirements.\nHere is a step-by-step tutorial from an expert instructing you how to complete it:\nIn this task, we want to use Airflow to trigger the data transfer of an Airbyte connection. Here are the detailed steps:\n1. In the opened Airbyte UI page, click the connection row in the main panel;\n2. In the new page, we can see the connection details:\n- in the url box, the format is:\n\"http://localhost:8000/workspaces/{workspace_id}/connections/{connection_id}/status\"\nwe can extract the connection id string for later usage (we will use ${connection_id} to represent it)\n- in the \"Enabled streams\" table, we can see 3 tables will be transferred and there is only table related to products (namely, `products`)\n3. Switch to the `DAGs - Airflow` web tab.\n4. We need to build two connections `airbyte_conn` and `file_conn` first. Click the \"Admin -> Connections\" button in the top menu bar.\n5. In the new page, click the \"+\" button under the Search box.\n6. For the following fields, type or select the corresponding values:\nConnection Id: airbyte_conn\nConnection Type: Airbyte\nHost: 172.17.0.1\nPort: 8000\nNote that, for the `Host` field, since both two containers (one for airbyte and one for airflow) are running in the docker environment, we can use the docker0 ip address (try `ifconfig docker0` in the terminal, which should usually be 172.17.0.1) or the ip address of the host (try `hostname -I | awk '{print $1}'` in the terminal) for communication if the special address `host.docker.internal` can not be identified.\n7. Since we disable the Airbyte basic auth in this local development, leave the two fields `Login` and `Password` empty.\n8. Now, we can save this connection `airbyte_conn` via click the \"Save\" button at the bottom.\n9. After coming back to the \"List Connection\" page, we click the \"+\" button again to create another connection `file_conn`.\n10. In this time, we fill in the following contents:\nConnection Id: file_conn\nConnection Type: File (path)\nJust leave the field `Path` empty, since we will specify it in the dag file.\n11. Click the button \"Save\" to return. We can see that, there are two connection rows in the main table.\n12. Now, click the Visual Studio Code application icon on the left panel of the desktop.\n13. Select and open the file with path `dags/faker_to_json_dag.py`.\n14. Type in the following code to overwrite the original snippets:\n```\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator\nfrom airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor\nfrom airflow.sensors.filesystem import FileSensor\nimport pendulum, os\n\nAIRBYTE_CONNECTION_ID = '${connection_id}' # replace this with your connection id\nSOURCE_RAW_PRODUCTS_FILE = '/tmp/airbyte_local/json_data/_airbyte_raw_products.jsonl'\nTARGET_RAW_PRODUCTS_FILE = f'{os.environ[\"AIRFLOW_HOME\"]}/data/raw_products.jsonl'\n\nwith DAG(dag_id='faker_to_json_dag',\n        start_date=pendulum.today('UTC').add(days=-1)\n    ) as dag:\n\n    trigger_airbyte_sync = AirbyteTriggerSyncOperator(\n        task_id='airbyte_trigger_sync',\n        airbyte_conn_id='airbyte_conn',\n        connection_id=AIRBYTE_CONNECTION_ID,\n        asynchronous=True\n    )\n\n    wait_for_sync_completion = AirbyteJobSensor(\n        task_id='airbyte_check_sync',\n        airbyte_conn_id='airbyte_conn',\n        airbyte_job_id=trigger_airbyte_sync.output\n    )\n\n    raw_products_file_sensor = FileSensor(\n        task_id='check_if_file_exists_task',\n        timeout=5,\n        filepath=SOURCE_RAW_PRODUCTS_FILE,\n        fs_conn_id='file_conn'\n    )\n\n    raw_products_file_ops= BashOperator(\n        task_id='copy_and_rename_raw_file',\n        bash_command=f'cp {SOURCE_RAW_PRODUCTS_FILE} {TARGET_RAW_PRODUCTS_FILE}'\n    )\n\n    trigger_airbyte_sync >> wait_for_sync_completion >>  raw_products_file_sensor >> raw_products_file_ops\n```\nNote that, for the special variable \"AIRBYTE_CONNECTION_ID = '${connection_id}'\", we need to replace '${connection_id}' with the concrete connection id previously found in the url address of Airbyte UI.\n15. Press Ctrl+S to save the modifications.\n16. Switch back to the Chromium/Chrome browser.\n17. Click the `DAGs` button in the top menu bar on web page `DAGs - Airflow`.\n18. Click the refresh button next to prompt \"Auto-refresh\" on top of the DAGs table. This will reload our changes to DAG `faker_to_json_dag`.\n19. Then, clcik the running button with icon \"â–¶\" under the Actions column in the main table.\n20. The synchronization will soon start and we can check the running details via clicking the text link `faker_to_json_dag` under the DAG column.\nYou can exactly follow the detailed plan above or proactively tackle the task based on the real-time environment interaction by yourself.",
    "source": [
        "https://airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together"
    ],
    "related_apps": [
        "chromium",
        "airbyte",
        "docker",
        "airflow",
        "vscode",
        "terminal"
    ],
    "tags": [
        "cli+gui",
        "data_ingestion_and_integration",
        "verbose"
    ],
    "action_number": 20,
    "config": [
        {
            "type": "copyfile_from_host_to_guest",
            "parameters": {
                "src": "evaluation_examples/examples/airbyte/d520508c-9b4a-4292-a9cd-0d65d6a2244e/connection.json",
                "dest": "/home/user/connection.json"
            }
        },
        {
            "type": "copyfile_from_host_to_guest",
            "parameters": {
                "src": "evaluation_examples/examples/airbyte/d520508c-9b4a-4292-a9cd-0d65d6a2244e/astro-airbyte-proj.zip",
                "dest": "/home/user/astro-airbyte-proj.zip"
            }
        },
        {
            "type": "script_and_execute",
            "parameters": {
                "src": "evaluation_examples/examples/airbyte/d520508c-9b4a-4292-a9cd-0d65d6a2244e/init.sh",
                "dest": "/home/user/init.sh"
            }
        },
        {
            "type": "google_chrome_browser",
            "parameters": {
                "debugging_port": 1337,
                "listening_port": 9222,
                "urls": [
                    "https://www.bing.com/"
                ]
            }
        },
        {
            "type": "astro_webui_init",
            "parameters": {
                "listening_port": 9222,
                "url": "http://localhost:8080",
                "actions": [
                    {
                        "type": "login",
                        "username": "admin",
                        "password": "admin"
                    }
                ]
            }
        },
        {
            "type": "airbyte_webui_init",
            "parameters": {
                "listening_port": 9222,
                "url": "http://localhost:8000",
                "actions": [
                    {
                        "type": "login",
                        "email": "anonym@gmail.com",
                        "company": "ANONYM"
                    }
                ]
            }
        }
    ],
    "evaluator": {
        "func": "check_include_exclude",
        "result": {
            "type": "vm_script_output",
            "src": "evaluation_examples/examples/airbyte/d520508c-9b4a-4292-a9cd-0d65d6a2244e/eval.sh",
            "dest": "/home/user/eval.sh"
        },
        "expected": {
            "type": "rule",
            "rules": {
                "include": [
                    "succeed"
                ],
                "exclude": [
                    "failed"
                ]
            }
        }
    },
    "counterpart": "ff70a7dd-ae40-4ab8-b958-dc2dfc1921e4"
}