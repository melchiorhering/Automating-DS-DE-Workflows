Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.html

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
After specifying the Destination Path, click on Set up destination.¬†</p><figcaption>Configure the Local JSON destination</figcaption><p>‚Äç</p><p>This will take you to a page to set up the connection. Set the replication frequency to <strong>Manual</strong>(since we will use Airflow to trigger Airbyte syncs rather than using Airbyte‚Äôs scheduler) and then click on <strong>Set up connection</strong>as highlighted in the image below.</p><figcaption>Specify connection settings</figcaption><p>‚Äç</p><p>Trigger a sync from the <strong>Sample Data (faker)</strong>source to the <strong>Local JSON</strong>output by clicking on <strong>Sync now</strong>as highlighted in the image below.</p><figcaption>Manually trigger a sync from the UI</figcaption><p>‚Äç</p><p>The sync should take a few seconds, at which point you should see that the sync has succeed as shown below.</p><figcaption>After the sync has completed</figcaption><p>‚Äç</p><p>You can now confirm if some sample data has been copied to the expected location. As previously mentioned, for this example the JSON data can be seen in <strong>/tmp/airbyte_local_json_from_faker</strong>. Because there were three streams generated, the following three JSON files should be available:¬†</p><code>_airbyte_raw_products.jsonl	
_airbyte_raw_users.jsonl
_airbyte_raw_purchases.jsonl
</code><p>You have now created a simple example connection in Airbyte which can be manually triggered. A manually triggered connection is ideal for situations where you wish to use an external orchestrator.¬†</p><p>In the next section you will see how to trigger a manual sync on this connection by hitting a REST endpoint directly. After that, you will see how Airflow can be used to hit that same endpoint to trigger synchronizations.¬†</p><h2>Test the API endpoints with cURL</h2><p>Before using the REST endpoint from within Airflow, it is useful to verify that it is working as expected.



Documentation Source:
airbyte.com/quickstart/airbyte-dbt-and-airflow-stack-with-bigquery.html

Documentation Title:
E-commerce Analytics Stack with Airbyte, dbt, Airflow (ADA) and BigQuery | Airbyte

Documentation Content:
Link Airbyte connection to the Airflow DAG</strong>:</h3><p>The last step being being able to execute the DAG in Airflow, is to include the connection ID from Airbyte:</p><ol><li>Visit the Airbyte UI at <a>http://localhost:8000/</a>.</li><li>In the "Connections" tab, select the "Faker to BigQuery" connection and copy its connection id from the URL.</li><li>Update the &lt;span class="text-style-code"&gt;connection_id&lt;/span&gt; in the &lt;span class="text-style-code"&gt;extract_data&lt;/span&gt; task within &lt;span class="text-style-code"&gt;orchestration/airflow/dags/elt_dag.py&lt;/span&gt; with this id.</li></ol><p>That's it! Airflow has been configured to work with dbt and Airbyte. üéâ</p><h2>6. Orchestrating with Airflow</h2><p>Now that everything is set up, it's time to run your data pipeline!</p><ol><li>In the Airflow UI, go to the "DAGs" section.</li><li>Locate &lt;span class="text-style-code"&gt;elt_dag&lt;/span&gt; and click on "Trigger DAG" under the "Actions" column.</li></ol><p>This will initiate the complete data pipeline, starting with the Airbyte sync from Faker to BigQuery, followed by dbt transforming the raw data into staging and marts models.



Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.html

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
For this tutorial I use the following default values:¬†</p><code>BASIC_AUTH_USERNAME=airbyte
BASIC_AUTH_PASSWORD=password
</code><p>Once Airbyte is running, in your browser type in localhost:8000, which should prompt you for a username and password as follows:</p><figcaption>Airbyte OSS¬†login prompt</figcaption><h2>Create a connection</h2><p>Create a connection that sends data from the <strong>Sample Data (Faker)</strong>source to the <strong>Local JSON</strong>(file system) output. Click on ‚ÄúCreate your first connection‚Äù as shown below:</p><figcaption>Create your first connection prompt</figcaption><p>‚Äç</p><p>You should then see an option to set up a source connection. Select the Faker source from the dropdown as shown below.</p><figcaption>Select Sample Data (Faker) as a source</figcaption><p>‚Äç</p><p>After selecting Sample Data as the source, you will see a screen that should look as follows. Click on <strong>Set up source</strong>as shown below.¬†</p><figcaption>Configure Sample Data (Faker) as a source</figcaption><p>‚Äç</p><p>You will then wait a few seconds for the Sample Data source to be verified, at which point you will be prompted to configure the destination that will be used for the connection. Select <strong>Local JSON</strong>as shown below:</p><figcaption>Select Local JSON as a destination</figcaption><p>‚Äç</p><p>After selecting Local JSON as the output, you will need to specify where the JSON files should be written. By default the path that you specify will be located inside <strong>/tmp/airbyte_local</strong>. In this tutorial I set the destination to <strong>/json_from_faker</strong>, which means that the data will be copied to<strong>/tmp/airbyte_local/json_from_faker</strong>on the localhost where Airbyte is running. After specifying the Destination Path, click on Set up destination.



Documentation Source:
airbyte.com/tutorials/how-to-use-airflow-and-airbyte-together.html

Documentation Title:
A step-by-step guide to setting up and configuring Airbyte and Airflow to work together | Airbyte

Documentation Content:
Because this is executed asynchronously, it immediately returns along with a job id that is used for determining the completion of the synchronization.</li><li><strong>wait_for_sync_completion</strong>: Uses <a>AirbyteJobSensor</a>to wait for Airbyte to complete the synchronization.¬†¬†¬†</li><li><strong>raw_products_file_sensor</strong>: Uses <a>FileSensor</a>to confirm that the file created by Airbyte exists. One of the files created by the <strong>Sample Data (Faker)</strong>source is called <strong>_airbyte_raw_products.jsonl</strong>, and this task waits for that file to exist.</li><li><strong>mv_raw_products_file</strong>: Uses <a>BashOperator</a>to rename the raw products file.</li></ol><p>The code which demonstrates these steps is given below.</p><code>from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator
from airflow.providers.airbyte.sensors.airbyte import AirbyteJobSensor
from airflow.sensors.filesystem import FileSensor
import pendulum

AIRBYTE_CONNECTION_ID = '[REPLACE THIS WITH YOUR CONNECTION ID]'
RAW_PRODUCTS_FILE = '/tmp/airbyte_local/json_from_faker/_airbyte_raw_products.jsonl'
COPY_OF_RAW_PRODUCTS = '/tmp/airbyte_local/json_from_faker/moved_raw_products.jsonl'

with DAG(dag_id='airbyte_example_airflow_dag',
        default_args={'owner': 'airflow'},
        schedule='@daily',
        start_date=pendulum.today('UTC').add(days=-1)
   ) as dag:

   trigger_airbyte_sync = AirbyteTriggerSyncOperator(
       task_id='airbyte_trigger_sync',
       airbyte_conn_id='airflow-call-to-airbyte-example',
       connection_id=AIRBYTE_CONNECTION_ID,
       asynchronous=True
   )

   wait_for_sync_completion = AirbyteJobSensor(
       task_id='airbyte_check_sync',
       airbyte_conn_id='airflow-call-to-airbyte-example',



